---
title: "Vance County EMS Station Analysis Report"
author: "Alayna Binder, Cindy Ju, Kathleen Zhang"
date: "2025-10-21"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    number_sections: true
    toc: false
fontsize: 11pt
geometry: margin=1in
df_print: kable
---

<!-- NOTE: To reference a figure in the report body, do @ref(fig:NameOfFig). -->

# Background

Without an effective distribution of emergency medical resources, counties risk delayed response times that can mean the difference between life and death. The current Vance County EMS configuration places all four ambulances at two existing bases in the Central and South districts, which has produced a systematic performance gap in the Northern district where calls incur materially longer response times. The county is considering establishing a third station in the North, at either a Near North or Far North site, together with a reallocation of the four ambulances across stations. We analyzed the historical calls to evaluate the 4 new scenarios, guided by three key criteria: (1) expected travel time under each scenario, (2) system load, measured through unit availability and concurrency of busy ambulances, and (3) how often the nominal closest station is actually able to serve a call at dispatch. We evaluated each call scenario using estimated travel times and the observed service durations and compare performance across the three criteria to determine which North location and ambulance allocation best improves coverage for the county.

# The Data

The dataset contains 489 EMS incidents recorded between January 1–25, 2024. One record corresponding to a transport to Duke Hospital was excluded, since it falls outside the county of interest and was abnormally long. In incidents with multiple patients transported in the same ambulance, duplicate records appear. Because the analysis concerns ambulance availability rather than patient count, we collapsed these duplicates so that each row represents a unique ambulance-level incident, while preserving separate rows when distinct ambulances responded to the same scene, since those affect system load.

For each incident, the data include the call location (latitude/longitude), dispatch, arrival, hospital, and clear timestamps, the assigned base under the current system, and Google-estimated travel times from each candidate station (South, Central, Near North, Far North). The Google estimates are provided under several traffic assumptions (optimistic, best-guess, pessimistic); we use the best-guess estimates in the simulation to reflect a typical operating regime. Hospital destination and service-duration fields are retained only to compute how long a unit remains busy after dispatch, which enters the system-load and queueing components of the analysis.

## Exploratory Data Analysis

We began by examining the spatial distribution of incidents. As shown in Figure 1, calls were densely concentrated near the current Central station, with a smaller secondary cluster around the South station. Calls also extended into the northern part of the county in a dispersed band, indicating sustained call activity in a region with no current station presence and motivating evaluation of a northern site.

Next, we compared the two candidate North locations using unadjusted Google travel times for calls originating in the North. As shown in Table \@ref(tab:tab-1), the Near North site was closer for over 90% of northern calls. Figure 2 further showed shorter response time, shorter total call duration, and faster hospital transport relative to Far North, providing initial evidence in favor of Near North conditional on establishing a station in the North.

We then examined unit-level workload. Figure 3 shows that centrally based units dominated utilization, with Medics 6 and 7 alone accounting for roughly one-third of observed busy time, while South-based units were seldom in service. This asymmetry reflects the earlier spatial concentration and indicates that the current layout leaves some units persistently near capacity.

To assess temporal pressure, we examined concurrent activity. Figure 4 illustrates episodes where multiple Central calls were active while a North call occurred simultaneously, implying that the nearest units were already engaged. We therefore quantified concurrency over the full window. As shown in Figure 5, Central exhibited both the highest typical concurrency and the widest tail. Although concurrency in the North was rare overall, 17.3% of North calls occurred during hours with at least three active Central calls, meaning that when North demand does arise, it often coincides with a period when nearby resources are already heavily committed.

# Modeling

## Data Simulation

The objectives are to compare the five candidate scenarios to determine the optimal options for station placement and ambulance allocation. Because we do not observe response times under unimplemented layouts, we conducted a simulation that generates response times for each scenario for every call in the cleaned dataset, resulting in five simulated observations per original incident. The scenarios under consideration are in Table \@ref(tab:tab-2). Dispatch rules governed how the simulation responded to different scenarios. 

From our EDA, we determined that we wanted to consider load scenarios. Therefore, each incoming call first checks whether any ambulances are currently available. If at least one unit is free, the system assigns the unit with the smallest estimated travel time, resulting in 0 wait time. Once assigned, each unit stays occupied for the observed duration of the call, and then becomes available again. If all units are busy when the call arrives, we assign the unit that is scheduled to become free soonest. The wait time there is the time difference between the call time and the departure time, which is the time the assigned unit finally becomes free. In both cases, the unit's subsequent availability is updated by adding the observed service duration to its departure time, making the unit busy for the duration of the incident. Total simulated response time is computed as wait time + travel time. We excluded simulated values above 2000 seconds to remove extreme, low-plausibility outcomes. We additionally created a “switched” indicator flagging whether the assigned station under a given scenario (S1–S4) differs from the baseline assignment (S0). Simulation results are in Table \@ref(tab:tab-3).

## Model Selection and Rationale

We chose to use a linear mixed model with a fixed effect on Scenario, which allowed us to calculate and test the difference in the mean simulated response time between our five scenarios (S0 through S4), as one of the primary objectives was determining changes in response time across different scenarios. We also included a random intercept for Incident_ID, allowing every unique incident to have its own baseline average response time that differs from the overall mean, even before accounting for the Scenario.

We fit a preliminary model with just these two components, but it had a lot of heteroscedasticity in the residuals plot. Therefore, we added a variance function that allows the remaining unexplained variability in response time to be different for each scenario. If a scenario has a high residual variance, it means that even after controlling for the mean, its response times are highly unpredictable or erratic. Meanwhile, if a scenario has a low residual variance, it indicates highly consistent service performance, which is generally desirable in a real-world context. Thus, our final model was:

$$
\begin{aligned}
\text{sim\_time}_{ij} &= \beta_0 + \sum_{k=1}^{4} \beta_k\,\mathbb{I}(\text{Scenario}_j = S_k) + u_i + \epsilon_{ij} \\
u_i &\sim \mathcal{N}(0,\sigma_u^2), \quad \epsilon_{ij} \sim \mathcal{N}(0,\sigma_j^2) \\
\text{where}\;\sigma_j^2 &= \sigma^2 \cdot \delta_j^2
\end{aligned}
$$

$\text{sim\_time}_{ij}$ is the simulated response time (seconds) for the $i$-th Incident ID under the $j$-th scenario. $u_i$ is the random intercept for the $i$-th Incident ID, accounting for the unique, unobserved baseline difficulty of that specific call. $\sigma^2_u$ is the variance of the random intercepts. $\sigma^2_j$ is the residual variance specific to the $j$-th scenario, which is modeled using our variance parameter $\delta^2_j$ that scales the baseline residual variance ($\sigma^2$) for Scenario $j$.

## Model Implementation and Evaluation

Linear mixed models were fit in R using the `nlme` package. We used the Restricted Maximum Likelihood (REML) method for estimation, which is standard practice for linear mixed models when comparing different variance structures. Additionally, the model was fit on a reduced dataset where incidents showing no variability in simulated response time across all five scenarios were dropped to ensure our coefficients primarily capture the effects of the scenario changes, not just baseline noise.

## Model Evaluation

First, while modelling, dropping the data where simulated time was the same across all scenarios resulted in a lower AIC and BIC of the model fitted on the reduced dataset compared to the non-reduced dataset. Next, after getting to our final model, we examined the residual plots to check model assumptions. A Q-Q plot can be found in Figure \@ref(fig:fig-7) and normalized Residuals vs Fitted Values plot can be found in Figure \@ref(fig:fig-8).

Due to anticipated heteroscedasticity, we fitted our model with a variance structure. We needed a normalized residuals plot to account for weighting due to our variance function, which applied weights to the residuals based on the Scenario to ensure the model fit is accurate despite the unequal variance. There did not appear to be any major patterns in the residuals, which are spread without a lot of heteroscedasticity. Additionally, there did not appear to be significant deviation from the Q-Q plot, indicating the assumption of normality is satisfied. 

We also looked at the Residuals vs Fitted Values plot by Scenario, where the points in red indicate calls where the assigned station under the simulation was different from the assigned station in our original dataset. Simulated scenarios generally showed more vertical spread than S0, meaning after controlling for mean response time and an incident's baseline difficulty, the unpredictability of the response time is typically higher in alternate scenarios. S3, which had the fastest mean response time, had less switches than S1 and S2 and a tighter spread, indicating better consistency.

## Model Results

To answer the research questions, we prioritized determining which of our 5 scenarios resulted in the shortest mean response time, as we felt that decreasing the average response time would be the best overall benefit to the community in terms of improving patient outcomes. Secondary considerations were the percent of simulated calls under 8 and 10 minutes, as more extreme wait times would lead to worse outcomes for patients, and another way to evaluate the best scenario was determining which one minimized "worse outcome" call times.

From our model results (Table \@ref(tab:tab-4)), the mean simulated total response time in S3 is estimated to be 60.96 seconds faster than our baseline, Scenario 0, with this being a significant difference with a p-value of 0.0062, well under our threshold of 0.05. The intercept term is the estimated mean simulated total response time for the reference group, Scenario 0, which is the current distribution of ambulances and stations. The mean response time in S0 is approximately 470.22 seconds or 7 minutes, 50 seconds. This indicates that the mean response time of S3 is estimated to be around 6 minutes, 48 seconds. All other scenarios had positive coefficients, indicating they resulted in longer mean response times than the baseline. Additionally, a boxplot revealed a wider spread of outliers in other scenarios compared to S3 (Figure \@ref(fig:fig-10)). Therefore, we concluded that the S3 station placement and ambulance allocation was ideal. 

# Conclusion, Shortcomings and Future Work

After evaluating five ambulance deployment layouts across the county by replaying the same 489 incidents under each scenario with a dispatch rule (sending the closest available unit, if none are free, dispact the next free unit when it is available), our analysis showed that Scenario 3, which locates one ambulance in the Near North, two ambulances in the Central, and one ambulance in the South, performed best. Across mean and median response times, Scenario 3 reduced average response time the most and produced the highest share of calls meeting eight and ten minute targets. These results are consistent with intuition, where a more balanced coverage of the county opposed to a centralized fleet lowers typical responses and extreme delays.

One limitation to our analysis is travel-time realism. We used Google's best-guess ETAs and treated them as fixed, so rush hour, weather, and road disruptions are not modeled, which underestimates our variability. Additionally, there was no priority handling and all calls were treated the same, so our model is limited in analyzing how our layouts affect critical cases vs. low-priority calls. In the future, additional work would involve adding time-of-day factors to travel by multiplying ETAs by peak/off-peak multipliers, and running two queues for emergency and non-emergency, allowing emergencies to jump ahead of any waiting non-emergency calls when waiting for the next dispatch.

\newpage

# Appendix

## Tables and Figures

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(ggrepel)
library(xtable)
library(knitr)
library(dplyr)
library(kableExtra)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
load("emsData.RData")

ems <- x |>
  select(-c( # remove unneeded columns
    eTT.Pe.So, eTT.Pe.Ce, eTT.Pe.NN, eTT.Pe.FN,
    eTT.BG.So, eTT.BG.Ce, eTT.BG.NN, eTT.BG.FN,
    eTT.Op.So, eTT.Op.Ce, eTT.Op.NN, eTT.Op.FN
  )) |>
  mutate(
    BASE.NAME = recode(BASE.NAME,
                       "Company 9" = "Central",
                       "Company 1" = "South"),
    BASE.NAME = factor(BASE.NAME, levels = c("Central","South")),
    REF.GRID = case_when(
      str_detect(REF.GRID, "North")   ~ "North",
      str_detect(REF.GRID, "Central") ~ "Central",
      str_detect(REF.GRID, "South")   ~ "South",
      TRUE ~ REF.GRID
    ),
    REF.GRID = factor(REF.GRID, levels = c("South","Central","North")),
    DISPATCH.PRIORITY.NAME = recode(DISPATCH.PRIORITY.NAME,
                                    "Non Emergency" = "Non-emergency",
                                    "Emergency" = "Emergency"),
    DISPATCH.PRIORITY.NAME = factor(DISPATCH.PRIORITY.NAME,
                                    levels = c("Emergency","Non-emergency")),
    went_hospital = REC.NAME != ""
  ) |>
  mutate(
    disp_to_enroute_min = as.numeric(timeToEnroute, units = "mins"),
    response_time_min = as.numeric(observedTT,    units = "mins"),
    on_scene_min = as.numeric(onSceneDur,    units = "mins"),
    to_hospital_min = as.numeric(toHospitalTT,  units = "mins"),
    at_hospital_min = as.numeric(atHospitalDur, units = "mins"),
    total_call_min = as.numeric(dispToClearTime, units = "mins")
  )
```

```{r fig-1, fig.cap = "EMS call density across Vance County (geographically)"}
#| echo: false
#| message: false


ems_plot <- data.frame(
  lon = as.numeric(x$REF.GPS.LON),
  lat = as.numeric(x$REF.GPS.LAT)
) |> na.omit() |>
  subset(lon > -78.55 & lon < -78.25 & lat > 36.15 & lat < 36.55)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

ggplot(ems_plot, aes(lon, lat)) +
  # hex bins look nicer than squares
  geom_hex(bins = 35, alpha = 0.95) +
  coord_fixed() +
  scale_fill_viridis_c(trans = "sqrt", name = "Calls") +
  # stations: clearer markers + labels
  geom_point(data = stations, aes(lon, lat),
             inherit.aes = FALSE, shape = 21, size = 3.8,
             stroke = 1, colour = "black", fill = "white") +
  geom_label_repel(data = stations, aes(lon, lat, label = name),
                   inherit.aes = FALSE, size = 3,
                   label.padding = unit(0.12, "lines"),
                   label.size = 0, seed = 1) +
  labs(title = "EMS call density",
       x = "Longitude", y = "Latitude") +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", size = 16),
    legend.key.height = unit(0.6, "cm")
  )
```

\newpage

```{r tab-1}
#| echo: false
#| message: false

ems <- ems |>
  mutate(
    closest_current = if_else(eTT.UA.Ce <= eTT.UA.So, "Central", "South"),
    closest_current = factor(closest_current, levels = c("South","Central")),
    closer_north = case_when(
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN <= eTT.UA.FN ~ "Near North",
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN >  eTT.UA.FN ~ "Far North",
      TRUE ~ NA_character_
    ),
    closer_north = factor(closer_north, levels = c("Near North","Far North"))
  )

# how often Near vs Far is closer for North calls
north_choice <- ems |>
  filter(REF.GRID == "North") |>
  count(closer_north) |>
  mutate(pct = n / sum(n))

north_choice |>
  kable(
    digits = 3,
    caption = "Proportion of North Calls Closer to Near vs Far North",
    col.names = c("Closer Station","Count","Proportion")
  )
```

```{r tab-2}
#| echo: false
#| message: false
# TABLE SHOWING SCENARIOS HERE
```


```{r fig-2, fig.cap = "Comparison of Near vs. Far North demand"}
#| echo: false
#| message: false

ems |>
  filter(REF.GRID == "North") |>
  select(closer_north, response_time_min, to_hospital_min, total_call_min) |>
  pivot_longer(-closer_north, names_to = "stage", values_to = "minutes") |>
  mutate(stage = recode(stage,
                        response_time_min = "Response Time",
                        to_hospital_min   = "Transport to Hospital",
                        total_call_min    = "Total Call Duration")) |>
  ggplot(aes(x = closer_north, y = minutes, fill = closer_north)) +
  geom_boxplot(width = 0.6, outlier.alpha = 0.15) +
  facet_wrap(~ stage, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = c("Near North" = "#1f77b4",
                               "Far North"  = "#ff7f0e")) +
  labs(
    x = NULL, 
    y = "Minutes", 
    title = "North Calls: Near vs. Far North"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "grey40")
  )
```

```{r, include=FALSE}

# --- 0) Clean datetimes & keep valid intervals ---
ems <- ems |>
  mutate(
    DT.DISP      = as_datetime(DT.DISP),
    DT.AVAILABLE = as_datetime(DT.AVAILABLE)
  ) |>
  filter(!is.na(VEH.GRID), !is.na(DT.DISP), !is.na(DT.AVAILABLE),
         DT.AVAILABLE >= DT.DISP)

# If two calls for the same ambulance overlap (due to logging quirks, delays, or bad timestamps), the code double-counts that unit as busy twice at the same time.
ems |>
  arrange(VEH.GRID, DT.DISP) |>
  group_by(VEH.GRID) |>
  mutate(overlap = DT.DISP < lag(DT.AVAILABLE)) |>
  filter(overlap == TRUE)

# Distinct vehicles present (sanity)
n_units <- n_distinct(ems$VEH.GRID)
if (n_units != 4) message("Note: dataset has ", n_units, " distinct VEH.GRID values.")

# Observation window (for utilization denominators)
t_start <- min(ems$DT.DISP, na.rm = TRUE)
t_end   <- max(ems$DT.AVAILABLE, na.rm = TRUE)
obs_seconds <- as.numeric(t_end - t_start, units = "secs")

# --- 1) Merge overlapping intervals within each vehicle ---
# Convert to numeric seconds for cummax, then back to POSIXct
ems_intervals <- ems |>
  transmute(VEH.GRID,
            start_num = as.numeric(DT.DISP),
            end_num   = as.numeric(DT.AVAILABLE)) |>
  arrange(VEH.GRID, start_num, end_num) |>
  group_by(VEH.GRID) |>
  mutate(
    prev_cummax_end = lag(cummax(end_num), default = first(start_num) - 1),
    new_block = start_num > prev_cummax_end,
    block_id  = cumsum(new_block)
  ) |>
  group_by(VEH.GRID, block_id) |>
  summarise(start_num = min(start_num),
            end_num   = max(end_num),
            .groups = "drop") |>
  mutate(start = as_datetime(start_num),
         end   = as_datetime(end_num)) |>
  select(VEH.GRID, start, end) |>
  arrange(VEH.GRID, start, end)

# --- 2) Build global event timeline (+1 at start, -1 at end) ---
events <- ems_intervals |>
  pivot_longer(c(start, end), names_to = "etype", values_to = "time") |>
  mutate(delta = if_else(etype == "start", 1L, -1L)) |>
  arrange(time, desc(delta))   # process -1 before +1 at identical timestamps

timeline <- events |>
  transmute(time, delta) |>
  arrange(time) |>
  mutate(active = cumsum(delta),
         time_next = lead(time),
         seg_sec   = as.numeric(time_next - time, "secs")) |>
  filter(!is.na(time_next), seg_sec > 0)

# --- 3) Duration-weighted summary of simultaneous busy vehicles (capped at 4) ---
timeline <- timeline |> mutate(active_capped = pmin(active, 4L))

simul_summary <- timeline |>
  group_by(active = active_capped) |>
  summarise(total_sec = sum(seg_sec), .groups = "drop") |>
  mutate(pct_of_time = total_sec / sum(total_sec)) |>
  arrange(active)

simul_summary
# active should now be 0..4 only

simul_summary |>
  mutate(
    `Active ambulances` = active,
    `Total time (hours)` = round(total_sec / 3600, 1),
    `% of time` = scales::percent(pct_of_time, accuracy = 0.1)
  ) |>
  select(`Active ambulances`, `Total time (hours)`, `% of time`) |>
  knitr::kable(format = "latex",
               align = "c",
               booktabs = TRUE,
               caption = "System Load: Simultaneous Busy Ambulances") |>
  kableExtra::kable_styling(full_width = FALSE, position = "center",
                            latex_options = c("hold_position","scale_down","striped"))

# Plot
ggplot(simul_summary, aes(factor(active), pct_of_time)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Simultaneous busy ambulances", y = "% of observed time",
       title = "System load: how many ambulances are busy simultaneously (capped at 4)")

# --- 5) Per-unit utilization (unchanged) ---
util_by_unit <- ems |>
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) |>
  group_by(VEH.GRID) |>
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups = "drop") |>
  mutate(utilization = busy_sec / obs_seconds) |>
  arrange(desc(utilization))

util_by_unit

# util_by_unit should already have: VEH.GRID, busy_sec, utilization
# If you also want color by base, grab a base label for each unit (mode per unit)
unit_base <- ems |>
  count(VEH.GRID, BASE.NAME) |>
  group_by(VEH.GRID) |>
  slice_max(n, with_ties = FALSE) |>
  ungroup() |>
  select(VEH.GRID, BASE.NAME)

util_plot <- util_by_unit |>
  left_join(unit_base, by = "VEH.GRID") |>
  mutate(
    VEH.GRID = factor(VEH.GRID, levels = rev(VEH.GRID[order(utilization)])),
    label = scales::percent(utilization, accuracy = 0.1)
  )

# medic and company matching 
by_medic_base <- ems |>
  count(VEH.GRID, BASE.NAME, name = "n_calls") |>
  group_by(VEH.GRID) |>
  mutate(pct = n_calls / sum(n_calls)) |>
  arrange(VEH.GRID, desc(n_calls)) |>
  ungroup()

# quick look just for Company 1
by_medic_base |> filter(BASE.NAME == "Company 1")

primary_base <- by_medic_base |>
  group_by(VEH.GRID) |>
  slice_max(pct, with_ties = FALSE) |>
  ungroup() |>
  select(VEH.GRID, primary_base = BASE.NAME, share_at_primary = pct)

util_by_base <- ems |>
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) |>
  group_by(BASE.NAME) |>
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups="drop") |>
  mutate(utilization = busy_sec / obs_seconds)

ggplot(by_medic_base, aes(x = reorder(VEH.GRID, -n_calls), y = pct, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Medic (VEH.GRID)", y = "Share of calls by base",
       title = "Where each medic runs from (call share by base)") +
  coord_flip() + theme_minimal(base_size = 12) +
  theme(panel.grid.major.y = element_blank(), legend.position = "top")
```

```{r, fig-3, fig.cap = "Percentage of ambulance utilization",}
#| echo: false
#| message: false

ggplot(util_plot, aes(x = VEH.GRID, y = utilization, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  geom_text(aes(label = label), hjust = -0.1, size = 3.6, color = "grey10") +
  coord_flip(clip = "off") +
  scale_y_continuous(labels = scales::percent, expand = expansion(mult = c(0, 0.10))) +
  scale_fill_brewer(palette = "Set2", na.value = "grey70", name = "Base") +
  labs(
    title = "Per-unit utilization over the observation window",
    subtitle = "Percentage of time each ambulance was busy (dispatch → available)",
    x = "Ambulance (VEH.GRID)",
    y = "Utilization"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(margin = margin(r = 10)),
    axis.title.x = element_text(margin = margin(t = 6)),
    plot.title = element_text(face = "bold"),
    legend.position = "top",
    legend.title = element_text(face = "bold"),
    plot.margin = margin(10, 30, 10, 10) # room for labels on the right
  )
```

```{r, include=FALSE}

TZ <- "America/New_York"

ems_clean <- ems |>
  filter(!is.na(DT.DISP), !is.na(DT.AVAILABLE), !is.na(REF.GRID)) |>
  mutate(
    # Normalize to a single local timezone
    dispatch_time   = with_tz(as.POSIXct(DT.DISP, tz = TZ), tzone = TZ),
    available_time  = with_tz(as.POSIXct(DT.AVAILABLE, tz = TZ), tzone = TZ),
    region          = REF.GRID
  ) |>
  filter(dispatch_time < available_time)

PAD_MIN <- 15L

# Max concurrency in [win_start, win_end) using PADDED intervals
max_conc_in_window_padded <- function(starts, ends, win_start, win_end, pad_min = PAD_MIN) {
  if (length(starts) == 0) return(0L)
  starts_pad <- starts - minutes(pad_min)
  ends_pad   <- ends   + minutes(pad_min)

  keep <- (starts_pad < win_end) & (ends_pad > win_start)
  if (!any(keep)) return(0L)

  starts_pad <- pmax(starts_pad[keep], win_start)
  ends_pad   <- pmin(ends_pad[keep],   win_end)

  events <- cbind(
    time  = c(starts_pad, ends_pad),
    delta = c(rep(1L, length(starts_pad)), rep(-1L, length(ends_pad)))
  )
  events <- events[order(events[, "time"], -events[, "delta"]), , drop = FALSE]

  conc <- 0L; maxc <- 0L
  for (i in seq_len(nrow(events))) {
    conc <- conc + events[i, "delta"]
    if (conc > maxc) maxc <- conc
  }
  as.integer(maxc)
}

# all local dates by dispatch date
ems_clean <- ems_clean |>
  mutate(dispatch_date = as.Date(dispatch_time, tz = TZ))

all_days   <- sort(unique(ems_clean$dispatch_date))
all_regions <- sort(unique(ems_clean$region))

# create a data frame of all day-hour-region combinations
grid <- expand.grid(
  region = all_regions,
  date   = all_days,
  hour   = 0:23,
  KEEP.OUT.ATTRS = FALSE,
  stringsAsFactors = FALSE
) |>
  mutate(
    hour_start = as.POSIXct(paste(date, sprintf("%02d:00:00", hour)), tz = TZ),
    hour_end   = hour_start + hours(1)
  )

# For each (region, date), restrict calls to those dispatched that date
concurrency_results <- grid |>
  group_by(region, date) |>
  group_modify(function(.df, key) {
    r <- key$region[1]; d <- key$date[1]

    inc <- ems_clean |>
      filter(region == r, dispatch_date == d)

    if (nrow(inc) == 0) {
      .df$max_concurrency <- 0L
      .df$num_calls <- 0L
      return(.df)
    }

    starts <- inc$dispatch_time
    ends   <- inc$available_time

    # Use PADDED intervals for both metrics
    .df$max_concurrency <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ max_conc_in_window_padded(starts, ends, ..1, ..2, PAD_MIN)
    )

    .df$num_calls <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ {
        s_pad <- starts - minutes(PAD_MIN)
        e_pad <- ends   + minutes(PAD_MIN)
        sum(s_pad < ..2 & e_pad > ..1)
      }
    )

    .df
  }) |>
  ungroup() |>
  mutate(
    hour_of_day = hour(hour_start),
    day_of_week = wday(date, label = TRUE),
    month       = month(date, label = TRUE)
  )

# Region-level summaries (now dates and hour_start dates are consistent)
summary_stats <- concurrency_results |>
  group_by(region) |>
  summarise(
    mean_max_concurrency   = mean(max_concurrency),
    median_max_concurrency = median(max_concurrency),
    max_concurrency_overall = max(max_concurrency),
    total_hours            = n(),
    hours_with_calls       = sum(num_calls > 0),
    .groups = "drop"
  )

# print(summary_stats)

```

```{r fig-4, fig.cap= "Example timeline of call status on January 16, 2024."}
#| echo: false
#| message: false


plot_daily_timeline_stacked <- function(data, target_date) {
  target_date <- as.Date(target_date, tz = TZ)

  daily_data <- data |>
    dplyr::filter(as.Date(dispatch_time, tz = TZ) == target_date) |>
    dplyr::arrange(region, dispatch_time) |>
    dplyr::group_by(region) |>
    dplyr::mutate(
      call_id        = dplyr::row_number(),
      # Actual (day-anchored)
      start_time     = as.POSIXct(paste(target_date, format(dispatch_time, "%H:%M:%S")), tz = TZ),
      end_time_raw   = as.POSIXct(paste(target_date, format(available_time, "%H:%M:%S")), tz = TZ),
      end_time       = dplyr::if_else(available_time < dispatch_time,
                                      end_time_raw + lubridate::days(1),
                                      end_time_raw),
      # Padded (day-anchored, with spillover handling)
      start_time_pad = start_time - lubridate::minutes(PAD_MIN),
      end_time_pad0  = end_time + lubridate::minutes(PAD_MIN),
      end_time_pad   = end_time_pad0,  # (already aligned to end_time which handled cross-midnight)
      duration_mins  = as.numeric(difftime(available_time, dispatch_time, units = "mins"))
    ) |>
    dplyr::ungroup()

  if (nrow(daily_data) == 0) {
    cat("No dispatches found on", as.character(target_date), "across any region.\n")
    return(invisible(NULL))
  }

  region_order <- daily_data |>
    dplyr::distinct(region) |>
    dplyr::arrange(region) |>
    dplyr::pull(region)

  daily_data$region <- factor(daily_data$region, levels = region_order)

  region_sizes <- daily_data |>
    dplyr::count(region, name = "n_calls") |>
    dplyr::arrange(region) |>
    dplyr::mutate(offset = dplyr::lag(cumsum(n_calls), default = 0L))

  daily_data <- daily_data |>
    dplyr::left_join(region_sizes, by = "region") |>
    dplyr::mutate(y_pos = offset + call_id)

  label_positions <- region_sizes |>
    dplyr::mutate(y_mid = offset + ceiling(n_calls / 2))

  x_min <- as.POSIXct(paste(target_date, "00:00:00"), tz = TZ)
  x_max <- max(
    max(daily_data$end_time_pad, na.rm = TRUE),
    as.POSIXct(paste(target_date, "23:59:59"), tz = TZ)
  )
  max_y <- max(daily_data$y_pos, na.rm = TRUE)

  library(ggplot2)
  p <- ggplot(daily_data, aes(y = y_pos, color = region)) +
    # --- PADDED interval (background) ---
    geom_segment(
      aes(x = start_time_pad, xend = end_time_pad, yend = y_pos),
      linewidth = 4, alpha = 0.25
    ) +
    geom_point(aes(x = start_time_pad), size = 3, alpha = 0.25) +
    geom_point(aes(x = end_time_pad),   size = 3, alpha = 0.25) +
    # --- ACTUAL interval (foreground) ---
    geom_segment(
      aes(x = start_time, xend = end_time, yend = y_pos),
      linewidth = 2, alpha = 0.9
    ) +
    geom_point(aes(x = start_time), size = 1.8) +
    geom_point(aes(x = end_time),   size = 1.8) +
    # Separators & region labels
    geom_hline(
      data = subset(region_sizes, offset > 0),
      aes(yintercept = offset + 0.5),
      linewidth = 0.3, alpha = 0.5, inherit.aes = FALSE
    ) +
    labs(
      title = "EMS Dispatch Timelines",
      subtitle = paste(format(target_date, "%B %d, %Y"), sprintf("— padding = ±%d min", PAD_MIN)),
      x = "Time of Day",
      y = "Call # (stacked)",
      color = "Region",
      caption = "Thick, faint band = padded window (±15m). Thin, solid band = actual call."
    ) +
    scale_x_datetime(
      limits = c(x_min, x_max),
      date_labels = "%H",
      date_breaks = "1 hour",
      expand = c(0.03, 0.03)
    ) +
    scale_y_continuous(
      breaks = seq(1, max_y, by = 1),
      minor_breaks = NULL,
      expand = c(0.02, 0.02)
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11),
      panel.grid.minor = element_blank(),
      axis.text.y = element_text(size = 7)
    )

  print(p)

  # Optional console summary (unchanged, still based on actual durations)
  # cat("\nSummary by region for", as.character(target_date), ":\n")
  # daily_data |>
  #   dplyr::group_by(region) |>
  #   dplyr::summarise(
  #     n_calls = dplyr::n(),
  #     avg_duration_min = round(mean(duration_mins), 1),
  #     med_duration_min = round(median(duration_mins), 1),
  #     min_duration_min = round(min(duration_mins), 1),
  #     max_duration_min = round(max(duration_mins), 1),
  #     .groups = "drop"
  #   ) |> print(n = Inf)
  # 
  # invisible(daily_data)
}

# Example:
plot_daily_timeline_stacked(ems_clean, "2024-01-16")

```

```{r, include=FALSE}

concurrency_results |>
  ggplot(aes(x = max_concurrency)) +
  geom_histogram() +
  facet_wrap(~ region)

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

mean(concurrency_central$overburdened)

```

```{r, fig-5, fig.cap="Shows the maximum concurrent number of calls within a given hour across the dataset. Most of the time there were no or very few concurrent calls, but Central was more prone to having concurrent calls."}
#| echo: false
#| message: false

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

counts <- concurrency_results |>
  mutate(max_concurrency = as.integer(max_concurrency)) |>
  count(region, max_concurrency, name = "n") |>
  # fill in missing combos so you always get 3 bars per x (even if some are 0)
  complete(region, max_concurrency = full_seq(range(max_concurrency, na.rm = TRUE), 1), fill = list(n = 0))

ggplot(counts, aes(x = factor(max_concurrency), y = n, fill = region)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.75) +
  labs(
    title = "Counts of Max Concurrency by Region",
    x = "Max concurrency (per hour)",
    y = "Number of hours"
  ) +
  theme_minimal()
```



```{r message=F, include=FALSE}
rm(list = ls())
library(tidyverse)
library(nlme)
library(sf)
library(dplyr)
library(tidyr)
library(purrr)
library(tibble)
set.seed(440)

load("emsData.RData")
```



```{r, include=FALSE}
ems <- x %>%
  mutate(Incident_ID = as.character(rownames(.)))
```

```{r, include=FALSE}
ems <- ems |>
  dplyr::arrange(DT.DISP, DT.AVAILABLE, BASE.NAME, Incident_ID) |>
  dplyr::distinct(Incident_ID, .keep_all = TRUE)
```

```{r, include=FALSE}
ems <- ems |>
  group_by(DT.DISP, BASE.NAME) |>
  mutate(
    Incident_ID = cur_group_id(),
    Patient_Count = n()
  ) |>
  slice(1) |>
  ungroup()
```





```{r, include=FALSE}
ems_simplified <- ems |>
  mutate(
    actual_travel_time = as.numeric(DT.ARRIVE - DT.DISP)
  ) |>
  select(Incident_ID, REF.GRID, DISPATCH.PRIORITY.NAME, REF.GPS.LAT, REF.GPS.LON, VEHCGPS, actual_travel_time) |>
  separate(VEHCGPS, into = c("VEH.GPS.LAT", "VEH.GPS.LON"), sep = ",", convert = TRUE) %>%
  mutate(
    VEH.GPS.LAT = as.numeric(trimws(VEH.GPS.LAT)),
    VEH.GPS.LON = as.numeric(trimws(VEH.GPS.LON)),
    # simple Euclidean distance in degrees
    distance = sqrt((REF.GPS.LAT - VEH.GPS.LAT)^2 + (REF.GPS.LON - VEH.GPS.LON)^2)
  ) |>
  filter(REF.GPS.LON > -78.7)
```

```{r, include=FALSE}
ems_pts = st_as_sf(ems_simplified, coords = c("REF.GPS.LON","REF.GPS.LAT"), crs = 4326)

# PARAMETERS
max_calls     <- 50      # split if a cell has > max_calls
cellsize_init <- 0.1    # starting cell size in degrees (your 0.1)
min_cellsize  <- 0.04   # don't split smaller than this (deg)
max_depth     <- 4      # safety guard against runaway splitting

xy <- st_coordinates(ems_pts)
x  <- xy[,1]
y  <- xy[,2]

bb   <- st_bbox(ems_pts)
xspan <- as.numeric(bb["xmax"] - bb["xmin"])
yspan <- as.numeric(bb["ymax"] - bb["ymin"])

nx <- max(1L, ceiling(xspan / cellsize_init))
ny <- max(1L, ceiling(yspan / cellsize_init))

x0 <- as.numeric(bb["xmin"])
y0 <- as.numeric(bb["ymin"])
x1 <- x0 + nx * cellsize_init
y1 <- y0 + ny * cellsize_init

cells <- list()
k <- 0L
for (i in 0:(nx-1L)) {
  for (j in 0:(ny-1L)) {
    k <- k + 1L
    xmin <- x0 + i * cellsize_init
    xmax <- xmin + cellsize_init
    ymin <- y0 + j * cellsize_init
    ymax <- ymin + cellsize_init
    idx  <- which(x >= xmin & x < xmax & y >= ymin & y < ymax)
    cells[[k]] <- list(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax,
                       idx=idx, depth=0L)
  }
}

leaves <- list()
queue  <- cells
cells  <- NULL

while (length(queue) > 0L) {
  node <- queue[[length(queue)]]; queue[[length(queue)]] <- NULL
  npts <- length(node$idx)
  width  <- node$xmax - node$xmin
  height <- node$ymax - node$ymin
  too_many   <- npts > max_calls
  can_split  <- (max(width, height) > min_cellsize) && (node$depth < max_depth)

  if (too_many && can_split) {
    mx <- (node$xmin + node$xmax) / 2
    my <- (node$ymin + node$ymax) / 2
    idx <- node$idx

    Q1 <- idx[ x[idx] >= node$xmin & x[idx] < mx   & y[idx] >= node$ymin & y[idx] <  my ]  # SW
    Q2 <- idx[ x[idx] >= mx        & x[idx] < node$xmax & y[idx] >= node$ymin & y[idx] <  my ]  # SE
    Q3 <- idx[ x[idx] >= node$xmin & x[idx] < mx   & y[idx] >= my         & y[idx] <= node$ymax ]# NW
    Q4 <- idx[ x[idx] >= mx        & x[idx] <= node$xmax & y[idx] >= my   & y[idx] <= node$ymax ]# NE

    depth2 <- node$depth + 1L
    queue[[length(queue)+1L]] <- list(xmin=node$xmin, xmax=mx, ymin=node$ymin, ymax=my, idx=Q1, depth=depth2)
    queue[[length(queue)+1L]] <- list(xmin=mx, xmax=node$xmax, ymin=node$ymin, ymax=my, idx=Q2, depth=depth2)
    queue[[length(queue)+1L]] <- list(xmin=node$xmin, xmax=mx, ymin=my, ymax=node$ymax, idx=Q3, depth=depth2)
    queue[[length(queue)+1L]] <- list(xmin=mx, xmax=node$xmax, ymin=my, ymax=node$ymax, idx=Q4, depth=depth2)
  } else {
    leaves[[length(leaves)+1L]] <- node
  }
}

rect_to_poly <- function(xmin,xmax,ymin,ymax){
  st_polygon(list(matrix(
    c(xmin,ymin,  xmin,ymax,  xmax,ymax,  xmax,ymin,  xmin,ymin),
    byrow=TRUE, ncol=2)))
}

polys <- lapply(leaves, function(nd) rect_to_poly(nd$xmin, nd$xmax, nd$ymin, nd$ymax))
sectors <- st_sf(
  sector_id = paste0("Q", seq_along(leaves)),
  n_calls   = vapply(leaves, function(nd) length(nd$idx), integer(1)),
  depth     = vapply(leaves, function(nd) nd$depth, integer(1)),
  geometry  = st_sfc(polys, crs = 4326)
)
```


```{r, include=FALSE}

ems_pts_with_sector <- st_join(ems_pts, sectors["sector_id"], join = st_within)

missing <- is.na(ems_pts_with_sector$sector_id)
if (any(missing)) {
  nearest_idx <- st_nearest_feature(ems_pts_with_sector[missing, ], sectors)
  ems_pts_with_sector$sector_id[missing] <- sectors$sector_id[nearest_idx]
}

ems_simplified$sector_id <- ems_pts_with_sector$sector_id

table(is.na(ems_simplified$sector_id))
head(ems_simplified$sector_id)
```
```{r, include=FALSE}
sector_counts <- ems_simplified %>%
  count(sector_id, name = "n_calls")

label_pts <- st_point_on_surface(sectors) %>%
  mutate(label = paste0(sector_id, "\n(n=", n_calls, ")"))

north_sectors <- c("Q22", "Q12", "Q5", "Q1", "Q23", "Q13", "Q6", "Q2")
south_sectors <- c("Q25", "Q21", "Q11", "Q4")
```

```{r, include=FALSE}
ems_simplified <- ems_simplified %>%
  filter(
    !is.na(actual_travel_time),
    !is.na(distance),
    !is.na(DISPATCH.PRIORITY.NAME),
    !is.na(sector_id),
    !is.na(REF.GRID),
    is.finite(actual_travel_time),
    is.finite(distance)
  ) %>%
  mutate(
    DISPATCH.PRIORITY.NAME = droplevels(factor(DISPATCH.PRIORITY.NAME)),
    sector_id              = droplevels(factor(sector_id)),
    REF.GRID               = droplevels(factor(REF.GRID))
  ) |>
  mutate(
    region = if_else(sector_id %in% north_sectors, "NORTH", if_else(sector_id %in% south_sectors, "SOUTH", "CENTRAL"))
  )

ems_pts = st_as_sf(ems_simplified, coords = c("REF.GPS.LON","REF.GPS.LAT"), crs = 4326)
```




```{r, include=FALSE}
eta_col_for <- function(station) {
  switch(station,
    "Central"   = "eTT.BG.Ce",
    "South"     = "eTT.BG.So",
    "NearNorth" = "eTT.BG.NN",
    "FarNorth"  = "eTT.BG.FN",
    stop(sprintf("Unknown station: %s", station))
  )
}
sec_between <- function(a, b) {
  as.numeric(difftime(a, b, units = "secs"))
}

simulate_dispatch <- function(dat, units_vec_named) {
  if (is.null(names(units_vec_named)) || anyNA(names(units_vec_named))) {
    stop("units_vec_named must be a named numeric vector, e.g. c(Central=3, South=1).")
  }
  if (sum(units_vec_named) <= 0) stop("No units in scenario.")

  dat <- dat[order(dat$DT.DISP, dat$DT.AVAILABLE, dat$Incident_ID), ]
  rownames(dat) <- NULL

  stations <- rep(names(units_vec_named), times = as.integer(units_vec_named))
  unit_idx <- ave(seq_along(stations), stations, FUN = seq_along)
  fleet <- data.frame(
    station   = stations,
    unit_id   = paste0(stations, "_", unit_idx),
    next_free = min(dat$DT.DISP, na.rm = TRUE) - 1,   # all free just before first call
    stringsAsFactors = FALSE
  )

  # Output containers
  n <- nrow(dat)
  out_station <- character(n)
  out_unit    <- character(n)
  out_wait    <- numeric(n)
  out_travel  <- numeric(n)

  # Sim loop
  for (i in seq_len(n)) {
    t_call <- dat$DT.DISP[i]
    if (is.na(t_call)) next

    # service duration as observed (dispatch -> available)
    svc <- sec_between(dat$DT.AVAILABLE[i], dat$DT.DISP[i])
    if (!is.finite(svc) || svc < 0) svc <- 0

    # per-unit ETA to this call
    eta_cols <- vapply(fleet$station, eta_col_for, character(1))
    eta_vals <- vapply(eta_cols, function(col) {
      if (!col %in% names(dat)) return(Inf)
      v <- dat[[col]][i]
      if (is.na(v)) Inf else as.numeric(v)   # assume ETAs already in seconds
    }, numeric(1))

    available_now <- fleet$next_free <= t_call
    available_now[is.na(available_now)] <- FALSE

    if (any(available_now)) {
      # choose among free units by minimal ETA (if all Inf, take the first free)
      idx_free <- which(available_now)
      best_idx <- if (any(is.finite(eta_vals[idx_free]))) {
        idx_free[ which.min(eta_vals[idx_free]) ]
      } else {
        idx_free[1]
      }
      wait_s   <- 0
      travel_s <- if (is.finite(eta_vals[best_idx])) eta_vals[best_idx] else 0
      depart   <- t_call
    } else {
      # nobody free: take earliest to free; waiting starts until that moment
      soonest  <- which.min(fleet$next_free)
      depart   <- fleet$next_free[soonest]
      wait_s   <- sec_between(depart, t_call)
      travel_s <- if (is.finite(eta_vals[soonest])) eta_vals[soonest] else 0
      best_idx <- soonest
    }

    # update busy time (use observed svc; do NOT add travel again to svc)
    fleet$next_free[best_idx] <- depart + wait_s + svc

    # record
    out_station[i] <- fleet$station[best_idx]
    out_unit[i]    <- fleet$unit_id[best_idx]
    out_wait[i]    <- wait_s
    out_travel[i]  <- travel_s
  }

  tibble::tibble(
    Incident_ID   = dat$Incident_ID,
    assigned_unit = out_unit,
    assigned_from = out_station,
    wait_secs     = out_wait,
    travel_secs   = out_travel,
    sim_time      = out_wait + out_travel   # dispatch→arrival + queue delay
  )
}
```

```{r, include=FALSE}

stopifnot(all(c("Incident_ID","DT.DISP","DT.AVAILABLE") %in% names(ems)))

# Scenario set (adjust counts as needed)
scenario_defs <- list(
  S0 = c(Central = 3, South     = 1),
  S1 = c(Central = 3, NearNorth = 1),
  S2 = c(Central = 3, FarNorth  = 1),
  S3 = c(Central = 2, NearNorth = 1, South = 1),
  S4 = c(Central = 2, FarNorth  = 1, South = 1)
)

sim_by_scenario <- imap_dfr(
  scenario_defs,
  ~ simulate_dispatch(ems, .x) %>% mutate(Scenario = .y),
  .id = NULL
)

sim_by_scenario <- sim_by_scenario %>%
  filter(is.finite(sim_time), sim_time >= 0, sim_time < 2000)

ems_simplified <- sim_by_scenario %>%
  left_join(ems_simplified, by = "Incident_ID") %>%
  relocate(Incident_ID, Scenario, sim_time, .before = dplyr::everything())
```

```{r, include=FALSE}

derive_orig_station <- function(ems) {
  if ("original_assignment" %in% names(ems)) {
    ems %>%
      transmute(Incident_ID, orig_station = as.character(original_assignment))
  } else if ("BASE.NAME" %in% names(ems)) {
    # Adjust this mapping to your system:
    # Example: Company 9 = Central, everything else = South
    ems %>%
      transmute(
        Incident_ID,
        orig_station = case_when(
          BASE.NAME == "Company 9" ~ "Central",
          TRUE ~ "South"
        )
      )
  } else {
    stop("Need either original_assignment or BASE.NAME in ems to derive original station.")
  }
}

# Ensure character Incident_ID in both frames
ems_simplified <- ems_simplified %>% mutate(Incident_ID = as.character(Incident_ID))

# 1) If an S0 baseline scenario exists, compare to that
if ("Scenario" %in% names(ems_simplified) &&
    any(ems_simplified$Scenario == "S0", na.rm = TRUE)) {

  baseline <- ems_simplified %>%
    filter(Scenario == "S0") %>%
    select(Incident_ID, base_assigned_from = assigned_from)

  ems_simplified <- ems_simplified %>%
    left_join(baseline, by = "Incident_ID") %>%
    mutate(
      switched = !is.na(assigned_from) & !is.na(base_assigned_from) &
                 assigned_from != base_assigned_from
    )

} else {
  ems <- ems %>% mutate(Incident_ID = as.character(Incident_ID))
  orig <- derive_orig_station(ems) %>% distinct(Incident_ID, .keep_all = TRUE)

  ems_simplified <- ems_simplified %>%
    left_join(orig, by = "Incident_ID") %>%
    mutate(
      switched = !is.na(assigned_from) & !is.na(orig_station) &
                 assigned_from != orig_station
    )
}
```

```{r, include=FALSE}
# some ambulances dont go to patient scene and are missing data, we won't
# consider them in our model
ems_model <- ems_simplified %>%
  mutate(
    Scenario = as.factor(Scenario),
    region   = as.factor(region),
    DISPATCH.PRIORITY.NAME = as.factor(DISPATCH.PRIORITY.NAME)
  ) %>%
  filter(
    is.finite(sim_time),
    !is.na(Scenario),
    !is.na(DISPATCH.PRIORITY.NAME),
    !is.na(region),
    !is.na(Incident_ID)
  ) %>%
  droplevels()

ems_model <- ems_model %>%
  mutate(DISPATCH.PRIORITY.NAME = relevel(factor(DISPATCH.PRIORITY.NAME),
                                          ref = "Non Emergency"))
```

```{r, tab-3}
#| echo: false
#| message: false

summ_tbl <- ems_model %>%
  group_by(Scenario) %>%
  summarise(
    n               = n(),
    mean_resp_s     = mean(sim_time),
    median_resp_s   = median(sim_time),
    # p90_resp_s      = quantile(sim_time, 0.90),
    # p95_resp_s      = quantile(sim_time, 0.95),
    pct_under_8min  = mean(sim_time <= 8*60),
    pct_under_10min = mean(sim_time <= 10*60),
    .groups = "drop"
  ) %>%
  arrange(mean_resp_s)

knitr::kable(
  summ_tbl,
  format = "latex",
  booktabs = TRUE,
  align = "lcccc",
  caption = "Table of summary statistics for simulation response time results by scenario."
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = c("hold_position","striped")
  )
```

```{r, include=FALSE}
scenario_means_long <- ems_model %>%
  # Group the data by the 'Scenario' variable
  group_by(Scenario) %>%
  # Calculate the mean of the simulated response time ('sim_time') within each group
  summarise(
    Mean_Response_Time = mean(sim_time, na.rm = TRUE),
    .groups = 'drop' # Ungroup after summarising
  )
```




```{r, include=FALSE}
ems_model <- ems_model %>%
  filter(sim_time < 2000)
```

```{r, include=FALSE}
dropped_ems_model <- ems_model |>
  group_by(Incident_ID) |>
  # Keep the group if the number of distinct simulated times is greater than 1
  filter(n_distinct(sim_time) > 1) |>
  ungroup()
```




```{r, include=FALSE}
m4 <- lme(
  fixed  = sim_time ~ Scenario,
  random = ~ 1 | Incident_ID,
  weights = varIdent(form = ~ 1 | Scenario),
  data   = dropped_ems_model,
  method = "REML"
)

summary(m4)
```

```{r, tab-4}
#| echo: false
#| message: false

# Fixed-effects coefficient table (from nlme::lme summary)
tt <- summary(m4)$tTable
coef_tbl <- tt %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Term")

knitr::kable(
  coef_tbl,
  format = "latex",
  booktabs = TRUE,
  align = "lcccc",
  caption = "Table of fixed effects coefficients from the final model."
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = c("hold_position","striped")
  )
```

```{r, tab-5}
#| echo: false
#| message: false

# Variance function (varIdent): residual SD multipliers by Scenario
var_coefs <- coef(m4$modelStruct$varStruct, unconstrained = FALSE)  # named multipliers (non-baseline levels)
levs <- levels(dropped_ems_model$Scenario)                           # model’s Scenario levels
sd_mult <- setNames(rep(1, length(levs)), levs)                      # baseline level has multiplier = 1
sd_mult[names(var_coefs)] <- var_coefs

var_df <- tibble::tibble(
  Scenario        = names(sd_mult),
  `SD multiplier` = as.numeric(sd_mult),
  `Var multiplier`= as.numeric(sd_mult)^2,
  .name_repair    = "minimal"
)

knitr::kable(
  var_df,
  format = "latex",
  booktabs = TRUE,
  align = "lcc",
  caption = "Table of variance function residual standard deviation multipliers by scenario."
) |>
  kableExtra::kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = c("hold_position","striped")
  )
```



```{r, fig-7, fig.cap="Q-Q plot of residuals. There is only a slight deviation in the Q-Q plot."}
#| echo: false
#| message: false

model_used <- getData(m4)

diag_df <- tibble(
  fitted    = fitted(m4),
  resid_raw = residuals(m4, type = "response"),    # in seconds
  resid_norm= residuals(m4, type = "normalized"),  # standardized (use these)
  Scenario  = model_used$Scenario,
  region    = model_used$region,
  sector_id = model_used$sector_id,
  observed  = model_used$sim_time,
)

# 2) QQ plot of residuals
qqnorm(diag_df$resid_norm); qqline(diag_df$resid_norm, col = "red", lwd = 2)
```

```{r, fig-8, fig.cap="Residuals vs Fitted Values plot. There does not appear to be heteroscedasticity or a fan pattern."}
#| echo: false
#| message: false
dat_used <- nlme::getData(m4)

tol <- 1e-9  # tolerance for "zero"

model_data <- data.frame(
  Fitted      = as.numeric(fitted(m4, level = 1)),               # group-level fit
  Residuals   = as.numeric(resid(m4, type = "normalized")),      # better w/ varIdent
  Incident_ID = dat_used$Incident_ID                             # aligned IDs
) %>%
  filter(!is.na(Residuals), abs(Residuals) > tol)

residual_plot <- ggplot(model_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", alpha = 0.5) +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Normalized Residuals"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

print(residual_plot)
```

```{r, fig-9, fig.cap="Residuals vs Fitted Values plot by scenario. There is variations in spread across the different scenarios."}
#| echo: false
#| message: false


plot_df <- dat_used |>
  dplyr::mutate(
    fit0   = as.numeric(fitted(m4, level = 0)),   # marginal (per Scenario mean)
    fit1   = as.numeric(fitted(m4, level = 1)),   # conditional (Incident-specific)
    r_norm = as.numeric(resid(m4, type = "normalized"))
  )

ggplot(plot_df, aes(fit1, r_norm, color = as.logical(switched))) +
  geom_point(alpha = 0.6, size = 1.3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  facet_wrap(~ Scenario) +
  scale_color_manual(values = c("grey50","red"), na.translate = FALSE) +
  labs(title = "Residuals vs Fitted by Scenario",
       x = "Fitted (level = 1)", y = "Normalized residuals", color = "Switched") +
  theme_minimal()
```

```{r fig-10, fig.cap="Plot of response time distributions by scenario. S3 had the lowest mean response time and a smaller spread of outliers"}
#| echo: false
#| message: false

ggplot(ems_model, aes(y = Scenario, x = sim_time)) +
  geom_boxplot(outlier.alpha = 0.25, width = 0.6, color = "gray30", fill = "white") +
  scale_x_continuous(
    "Total Response Time (minutes)",
    breaks = seq(0, 2000, 150),
    labels = function(x) round(x / 60, 1) # Divide by 60 and round to 1 decimal place
  ) +
  labs(
    title = "Response Time Distributions by Scenario",
    subtitle = "Distribution of total (travel + wait) response time per scenario",
    y = "Scenario"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40"),
    axis.title.x = element_text(face = "bold", margin = margin(t = 8)),
    axis.title.y = element_text(face = "bold", margin = margin(r = 8))
  )
```
