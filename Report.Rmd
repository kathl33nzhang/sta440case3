---
title: "Vance County EMS Station Analysis Report"
author: "Alayna Binder, Cindy Ju, Kathleen Zhang"
date: "2025-10-21"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    # toc: true
fontsize: 11pt
geometry: margin=1in
df_print: kable
---

<!-- NOTE: To reference a figure in the report body, do @ref(fig:NameOfFig). -->

# Background

The current Vance County EMS configuration places all four ambulances at two existing bases in the Central and South districts, which has produced a systematic performance gap in the Northern district where calls incur materially longer response times. The county is considering establishing a third station in the North, at either a Near North or Far North site, together with a reallocation of the four ambulances across stations.

We analyzed the historical calls to evaluate the 4 new scenarios, guided by three key criteria: (1) expected travel time under each scenario, (2) system load, measured through unit availability and concurrency of busy ambulances, and (3) how often the nominal closest station is actually able to serve a call at dispatch.

We evaluated each call scenario using estimated travel times and the observed service durations and compare performance across the three criteria to determine which North location and ambulance allocation best improves coverage for the county.

## The Data

The dataset contains 489 EMS incidents recorded between January 1–25, 2024. One record corresponding to a transport to Duke Hospital was excluded, since it falls outside the county of interest and was abnormally long. In incidents with multiple patients transported in the same ambulance, duplicate records appear. Because the analysis concerns ambulance availability rather than patient count, we collapsed these duplicates so that each row represents a unique ambulance-level incident, while preserving separate rows when distinct ambulances responded to the same scene, since those affect system load.

For each incident, the data include the call location (latitude/longitude), dispatch, arrival, hospital, and clear timestamps, the assigned base under the current system, and Google-estimated travel times from each candidate station (South, Central, Near North, Far North). The Google estimates are provided under several traffic assumptions (optimistic, best-guess, pessimistic); we use the best-guess estimates in the simulation to reflect a typical operating regime. Hospital destination and service-duration fields are retained only to compute how long a unit remains busy after dispatch, which enters the system-load and queueing components of the analysis.

## Exploratory Data Analysis

We began by examining the spatial distribution of incidents. As shown in Figure 1, calls were densely concentrated near the current Central station, with a smaller secondary cluster around the South station. Calls also extended into the northern part of the county in a dispersed band, indicating sustained call activity in a region with no current station presence and motivating evaluation of a northern site.

Next, we compared the two candidate North locations using unadjusted Google travel times for calls originating in the North. As shown in Table 1, the Near North site was closer for over 90% of northern calls. Figure 2 further showed shorter response time, shorter total call duration, and faster hospital transport relative to Far North, providing initial evidence in favor of Near North conditional on establishing a station in the North.

We then examined unit-level workload. Figure 3 shows that centrally based units dominated utilization, with Medics 6 and 7 alone accounting for roughly one-third of observed busy time, while South-based units were seldom in service. This asymmetry reflects the earlier spatial concentration and indicates that the current layout leaves some units persistently near capacity.

To assess temporal pressure, we examined concurrent activity. Figure 4 illustrates episodes where multiple Central calls were active while a North call occurred simultaneously, implying that the nearest units were already engaged. We therefore quantified concurrency over the full window. As shown in Figure 5, Central exhibited both the highest typical concurrency and the widest tail. Although concurrency in the North was rare overall, 17.3% of North calls occurred during hours with at least three active Central calls, meaning that when North demand does arise, it often coincides with a period when nearby resources are already heavily committed.

# Modeling

## Simulating the Data

The objectives are to compare the five candidate scenarios to determine the optimal options for station placement and ambulance allocation. Because we do not observe response times under unimplemented layouts, we conducted a simulation that generates response times for each scenario for every call in the cleaned dataset, resulting in five simulated observations per original incident.

We constructed a set of Scenario Dispatch Rules to govern how incidents are handled under each configuration. From exploratory analysis, we determined that the simulation must allow for the possibility that no ambulance is free when a call arrives. Under the rules, each incoming call first checks whether any units are available. If at least one unit is free, the system assigns the closest available one, using Google’s best-guess estimated travel time from station to call location; this yields a wait time of zero. If all units are busy, the call enters a queue and waits until the next ambulance becomes free; that waiting time is then added to the total response time. Once dispatched, a unit remains occupied for the observed duration of the incident and then becomes available for future calls.

Wait time is therefore determined entirely by availability at arrival. If one or more units are free, the closest (minimum ETA) unit is dispatched immediately with zero wait. If all units are busy, the unit scheduled to become free soonest is assigned, and the wait time is the difference between the call time and that unit’s release time. In both cases, subsequent availability is updated by adding the observed service duration to that unit’s departure time.

Total simulated response time is computed as wait time + travel time. We excluded simulated values above 2000 seconds to remove extreme, low-plausibility outcomes. We additionally created a “switched” indicator flagging whether the assigned station under a given scenario (S1–S4) differs from the baseline assignment (S0).

## Model Selection and Rationale

We chose to use a linear mixed model with a fixed effect on Scenario, which allowed us to calculate and test the difference in the mean simulated response time between our five Scenarios (S0 through S4), as one of the primary objectives was determining changes in response time across different scenarios.

We also included a random intercept for Incident_ID, allowing every unique incident to have its own baseline average response time that differs from the overall mean, even before accounting for the Scenario.

We fit a preliminary model with just these two components, but it had a lot of heteroscedasticity in the residuals plot. Therefore, we added a variance function that allows the remaining unexplained variability in response time to be different for each Scenario. If a scenario has a high residual variance, it means that even after controlling for the mean, its response times are highly unpredictable or erratic. Meanwhile, if a scenario has a low residual variance, it indicates highly consistent service performance, which is generally desirable in a real-world context.

Thus, our final model was:

\[
\mathrm{sim\_time}_{ij}
= \beta_0 + \sum_{k=1}^4 \beta_k I(\text{Scenario}_j = S_k) + u_i + \epsilon_{ij}
\]

$$
u_i \sim N(0, \sigma^2_u)\text{, }\epsilon_{ij} \sim N(0, \sigma^2)
$$

where $\sigma^2_j = \sigma^2 \cdot\delta^2_j$

$sim\_time_{ij}$ is the simulated response time for the $i$-th Incident ID under the $j$-th Scenario.

$u_i$ is the random intercept for the $i$-th Incident ID. $\sigma^2_j$ is the residual variance specific to the $j$-th Scenario, which is impacted by our variance parameter $\delta_j^2$ that scales the baseline variance for Scenario $j$.

## Model Implementation and Evaluation

Linear mixed models were fit in R using the `nlme` package. Additionally, we ended up dropping data where simulated time was the same across all scenarios in order to focus on the effect of changes resulting from the different Scenarios.

Dropping the data where simulated time was the same across all scenarios resulted in a lower AIC and BIC of the model fitted on the reduced dataset compared to the non-reduced dataset.

We also examined a QQ plot which appears approximately normal. Our Normalized Residuals vs Fitted Values plot of the total response time appears randomly distributed.

We also looked at the Residuals vs Fitted by Scenario, where the points in red indicate calls where the assigned station under the simulation was different from the assigned station in our original dataset.

## Model Results

The intercept term is the estimated mean simulated total response time for the reference group, Scenario 0, which is the current distribution of ambulances and stations. The mean response time in S0 is approximately 470.22 seconds or 7 minutes, 50 seconds. This value is highly significant, with a p-value \< 0.001 .

The mean simulated total response time in Scenario 3 is estimated to be 60.96 seconds faster than in Scenario 0, with this being a significant difference.

# Conclusion, Shortcomings and Future Work

After evaluating five ambulance deployment layouts across the county by replaying the same 489 incidents under each scenario with a dispatch rule (sending the closest available unit, if none are free, dispact the next free unit when it is available), our analysis showed that Scenario 3, which locates one ambulance in the Near North, two ambulances in the Central, and one ambulance in the South, perfomed best. Across mean and median response times, Scenario 3 reduced average response time the most and produced the highest share of calls meeting eight and ten minute targets. These results are consistent with intuition, where a more balanced coverage of the county opposed to a centralized fleet lowers typical responses and extreme delays.

However, one limitation to our analysis is the travel-time realism- we used Google's unadjusted ETAs and treated them as fixed, so rush hour, weather, and road disruptions are not modeled, which underestimates our variability. Additionally, there was no priority handling and all calls were treated the same, so our model is limited in analyzing how our layouts affect critical cases vs. low-priority calls. In the future, additional work would involve adding time-of-day factors to travel by multiplying ETAs by peak/off-peak multipliers, and running two queues for emergency and non-emergency, allowing emergencies to jump ahead of any waiting non-emergency calls when waiting for the next dispatch.

\newpage

# Appendix

## Tables and Figures

<!-- Make sure to caption all tables/figures. -->

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(ggrepel)
library(xtable)
library(knitr)
library(dplyr)
library(kableExtra)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
load("emsData.RData")

ems <- x |>
  select(-c( # remove unneeded columns
    eTT.Pe.So, eTT.Pe.Ce, eTT.Pe.NN, eTT.Pe.FN,
    eTT.BG.So, eTT.BG.Ce, eTT.BG.NN, eTT.BG.FN,
    eTT.Op.So, eTT.Op.Ce, eTT.Op.NN, eTT.Op.FN
  )) |>
  mutate(
    BASE.NAME = recode(BASE.NAME,
                       "Company 9" = "Central",
                       "Company 1" = "South"),
    BASE.NAME = factor(BASE.NAME, levels = c("Central","South")),
    REF.GRID = case_when(
      str_detect(REF.GRID, "North")   ~ "North",
      str_detect(REF.GRID, "Central") ~ "Central",
      str_detect(REF.GRID, "South")   ~ "South",
      TRUE ~ REF.GRID
    ),
    REF.GRID = factor(REF.GRID, levels = c("South","Central","North")),
    DISPATCH.PRIORITY.NAME = recode(DISPATCH.PRIORITY.NAME,
                                    "Non Emergency" = "Non-emergency",
                                    "Emergency" = "Emergency"),
    DISPATCH.PRIORITY.NAME = factor(DISPATCH.PRIORITY.NAME,
                                    levels = c("Emergency","Non-emergency")),
    went_hospital = REC.NAME != ""
  ) |>
  mutate(
    disp_to_enroute_min = as.numeric(timeToEnroute, units = "mins"),
    response_time_min = as.numeric(observedTT,    units = "mins"),
    on_scene_min = as.numeric(onSceneDur,    units = "mins"),
    to_hospital_min = as.numeric(toHospitalTT,  units = "mins"),
    at_hospital_min = as.numeric(atHospitalDur, units = "mins"),
    total_call_min = as.numeric(dispToClearTime, units = "mins")
  )
```

**Figure 1: Density of Calls in Vance County**

```{r label = "eda1", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

ems_plot <- data.frame(
  lon = as.numeric(x$REF.GPS.LON),
  lat = as.numeric(x$REF.GPS.LAT)
) |> na.omit() |>
  subset(lon > -78.55 & lon < -78.25 & lat > 36.15 & lat < 36.55)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

ggplot(ems_plot, aes(lon, lat)) +
  # hex bins look nicer than squares
  geom_hex(bins = 35, alpha = 0.95) +
  coord_fixed() +
  scale_fill_viridis_c(trans = "sqrt", name = "Calls") +
  # stations: clearer markers + labels
  geom_point(data = stations, aes(lon, lat),
             inherit.aes = FALSE, shape = 21, size = 3.8,
             stroke = 1, colour = "black", fill = "white") +
  geom_label_repel(data = stations, aes(lon, lat, label = name),
                   inherit.aes = FALSE, size = 3,
                   label.padding = unit(0.12, "lines"),
                   label.size = 0, seed = 1) +
  labs(title = "EMS call density",
       x = "Longitude", y = "Latitude") +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", size = 16),
    legend.key.height = unit(0.6, "cm")
  )
```

\newpage

**Table 1: Proportion of North Calls Closer to Near vs Far North**

```{r label = "eda2_1", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

ems <- ems |>
  mutate(
    closest_current = if_else(eTT.UA.Ce <= eTT.UA.So, "Central", "South"),
    closest_current = factor(closest_current, levels = c("South","Central")),
    closer_north = case_when(
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN <= eTT.UA.FN ~ "Near North",
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN >  eTT.UA.FN ~ "Far North",
      TRUE ~ NA_character_
    ),
    closer_north = factor(closer_north, levels = c("Near North","Far North"))
  )

# how often Near vs Far is closer for North calls
north_choice <- ems |>
  filter(REF.GRID == "North") |>
  count(closer_north) |>
  mutate(pct = n / sum(n))

north_choice |>
  kable(
    digits = 3,
    col.names = c("Closer Station","Count","Proportion")
  )
```

**Figure 2: Comparison of Near vs. Far North Demand**

```{r label = "eda2_2", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

ems |>
  filter(REF.GRID == "North") |>
  select(closer_north, response_time_min, to_hospital_min, total_call_min) |>
  pivot_longer(-closer_north, names_to = "stage", values_to = "minutes") |>
  mutate(stage = recode(stage,
                        response_time_min = "Response Time",
                        to_hospital_min   = "Transport to Hospital",
                        total_call_min    = "Total Call Duration")) |>
  ggplot(aes(x = closer_north, y = minutes, fill = closer_north)) +
  geom_boxplot(width = 0.6, outlier.alpha = 0.15) +
  facet_wrap(~ stage, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = c("Near North" = "#1f77b4",
                               "Far North"  = "#ff7f0e")) +
  labs(
    x = NULL, 
    y = "Minutes", 
    title = "North Calls: Near vs. Far North"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "grey40")
  )
```

\newpage

**Figure 3: Percentage of Ambulance Utilization**

```{r, include=FALSE}

# --- 0) Clean datetimes & keep valid intervals ---
ems <- ems |>
  mutate(
    DT.DISP      = as_datetime(DT.DISP),
    DT.AVAILABLE = as_datetime(DT.AVAILABLE)
  ) |>
  filter(!is.na(VEH.GRID), !is.na(DT.DISP), !is.na(DT.AVAILABLE),
         DT.AVAILABLE >= DT.DISP)

# If two calls for the same ambulance overlap (due to logging quirks, delays, or bad timestamps), the code double-counts that unit as busy twice at the same time.
ems |>
  arrange(VEH.GRID, DT.DISP) |>
  group_by(VEH.GRID) |>
  mutate(overlap = DT.DISP < lag(DT.AVAILABLE)) |>
  filter(overlap == TRUE)

# Distinct vehicles present (sanity)
n_units <- n_distinct(ems$VEH.GRID)
if (n_units != 4) message("Note: dataset has ", n_units, " distinct VEH.GRID values.")

# Observation window (for utilization denominators)
t_start <- min(ems$DT.DISP, na.rm = TRUE)
t_end   <- max(ems$DT.AVAILABLE, na.rm = TRUE)
obs_seconds <- as.numeric(t_end - t_start, units = "secs")

# --- 1) Merge overlapping intervals within each vehicle ---
# Convert to numeric seconds for cummax, then back to POSIXct
ems_intervals <- ems |>
  transmute(VEH.GRID,
            start_num = as.numeric(DT.DISP),
            end_num   = as.numeric(DT.AVAILABLE)) |>
  arrange(VEH.GRID, start_num, end_num) |>
  group_by(VEH.GRID) |>
  mutate(
    prev_cummax_end = lag(cummax(end_num), default = first(start_num) - 1),
    new_block = start_num > prev_cummax_end,
    block_id  = cumsum(new_block)
  ) |>
  group_by(VEH.GRID, block_id) |>
  summarise(start_num = min(start_num),
            end_num   = max(end_num),
            .groups = "drop") |>
  mutate(start = as_datetime(start_num),
         end   = as_datetime(end_num)) |>
  select(VEH.GRID, start, end) |>
  arrange(VEH.GRID, start, end)

# --- 2) Build global event timeline (+1 at start, -1 at end) ---
events <- ems_intervals |>
  pivot_longer(c(start, end), names_to = "etype", values_to = "time") |>
  mutate(delta = if_else(etype == "start", 1L, -1L)) |>
  arrange(time, desc(delta))   # process -1 before +1 at identical timestamps

timeline <- events |>
  transmute(time, delta) |>
  arrange(time) |>
  mutate(active = cumsum(delta),
         time_next = lead(time),
         seg_sec   = as.numeric(time_next - time, "secs")) |>
  filter(!is.na(time_next), seg_sec > 0)

# --- 3) Duration-weighted summary of simultaneous busy vehicles (capped at 4) ---
timeline <- timeline |> mutate(active_capped = pmin(active, 4L))

simul_summary <- timeline |>
  group_by(active = active_capped) |>
  summarise(total_sec = sum(seg_sec), .groups = "drop") |>
  mutate(pct_of_time = total_sec / sum(total_sec)) |>
  arrange(active)

simul_summary
# active should now be 0..4 only

simul_summary |>
  mutate(
    `Active ambulances` = active,
    `Total time (hours)` = round(total_sec / 3600, 1),
    `% of time` = scales::percent(pct_of_time, accuracy = 0.1)
  ) |>
  select(`Active ambulances`, `Total time (hours)`, `% of time`) |>
  knitr::kable(format = "latex",
               align = "c",
               booktabs = TRUE,
               caption = "System Load: Simultaneous Busy Ambulances") |>
  kableExtra::kable_styling(full_width = FALSE, position = "center",
                            latex_options = c("hold_position","scale_down","striped"))

# Plot
ggplot(simul_summary, aes(factor(active), pct_of_time)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Simultaneous busy ambulances", y = "% of observed time",
       title = "System load: how many ambulances are busy simultaneously (capped at 4)")

# --- 5) Per-unit utilization (unchanged) ---
util_by_unit <- ems |>
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) |>
  group_by(VEH.GRID) |>
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups = "drop") |>
  mutate(utilization = busy_sec / obs_seconds) |>
  arrange(desc(utilization))

util_by_unit

# util_by_unit should already have: VEH.GRID, busy_sec, utilization
# If you also want color by base, grab a base label for each unit (mode per unit)
unit_base <- ems |>
  count(VEH.GRID, BASE.NAME) |>
  group_by(VEH.GRID) |>
  slice_max(n, with_ties = FALSE) |>
  ungroup() |>
  select(VEH.GRID, BASE.NAME)

util_plot <- util_by_unit |>
  left_join(unit_base, by = "VEH.GRID") |>
  mutate(
    VEH.GRID = factor(VEH.GRID, levels = rev(VEH.GRID[order(utilization)])),
    label = scales::percent(utilization, accuracy = 0.1)
  )

# medic and company matching 
by_medic_base <- ems |>
  count(VEH.GRID, BASE.NAME, name = "n_calls") |>
  group_by(VEH.GRID) |>
  mutate(pct = n_calls / sum(n_calls)) |>
  arrange(VEH.GRID, desc(n_calls)) |>
  ungroup()

# quick look just for Company 1
by_medic_base |> filter(BASE.NAME == "Company 1")

primary_base <- by_medic_base |>
  group_by(VEH.GRID) |>
  slice_max(pct, with_ties = FALSE) |>
  ungroup() |>
  select(VEH.GRID, primary_base = BASE.NAME, share_at_primary = pct)

util_by_base <- ems |>
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) |>
  group_by(BASE.NAME) |>
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups="drop") |>
  mutate(utilization = busy_sec / obs_seconds)

ggplot(by_medic_base, aes(x = reorder(VEH.GRID, -n_calls), y = pct, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Medic (VEH.GRID)", y = "Share of calls by base",
       title = "Where each medic runs from (call share by base)") +
  coord_flip() + theme_minimal(base_size = 12) +
  theme(panel.grid.major.y = element_blank(), legend.position = "top")
```

```{r label = "eda3", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

ggplot(util_plot, aes(x = VEH.GRID, y = utilization, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  geom_text(aes(label = label), hjust = -0.1, size = 3.6, color = "grey10") +
  coord_flip(clip = "off") +
  scale_y_continuous(labels = scales::percent, expand = expansion(mult = c(0, 0.10))) +
  scale_fill_brewer(palette = "Set2", na.value = "grey70", name = "Base") +
  labs(
    title = "Per-unit utilization over the observation window",
    subtitle = "Percentage of time each ambulance was busy (dispatch → available)",
    x = "Ambulance (VEH.GRID)",
    y = "Utilization"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(margin = margin(r = 10)),
    axis.title.x = element_text(margin = margin(t = 6)),
    plot.title = element_text(face = "bold"),
    legend.position = "top",
    legend.title = element_text(face = "bold"),
    plot.margin = margin(10, 30, 10, 10) # room for labels on the right
  )
```

\newpage

**Figure 4: Examining Overburdening Issue**

```{r, include=FALSE}

TZ <- "America/New_York"

ems_clean <- ems |>
  filter(!is.na(DT.DISP), !is.na(DT.AVAILABLE), !is.na(REF.GRID)) |>
  mutate(
    # Normalize to a single local timezone
    dispatch_time   = with_tz(as.POSIXct(DT.DISP, tz = TZ), tzone = TZ),
    available_time  = with_tz(as.POSIXct(DT.AVAILABLE, tz = TZ), tzone = TZ),
    region          = REF.GRID
  ) |>
  filter(dispatch_time < available_time)

PAD_MIN <- 15L

# Max concurrency in [win_start, win_end) using PADDED intervals
max_conc_in_window_padded <- function(starts, ends, win_start, win_end, pad_min = PAD_MIN) {
  if (length(starts) == 0) return(0L)
  starts_pad <- starts - minutes(pad_min)
  ends_pad   <- ends   + minutes(pad_min)

  keep <- (starts_pad < win_end) & (ends_pad > win_start)
  if (!any(keep)) return(0L)

  starts_pad <- pmax(starts_pad[keep], win_start)
  ends_pad   <- pmin(ends_pad[keep],   win_end)

  events <- cbind(
    time  = c(starts_pad, ends_pad),
    delta = c(rep(1L, length(starts_pad)), rep(-1L, length(ends_pad)))
  )
  events <- events[order(events[, "time"], -events[, "delta"]), , drop = FALSE]

  conc <- 0L; maxc <- 0L
  for (i in seq_len(nrow(events))) {
    conc <- conc + events[i, "delta"]
    if (conc > maxc) maxc <- conc
  }
  as.integer(maxc)
}

# all local dates by dispatch date
ems_clean <- ems_clean |>
  mutate(dispatch_date = as.Date(dispatch_time, tz = TZ))

all_days   <- sort(unique(ems_clean$dispatch_date))
all_regions <- sort(unique(ems_clean$region))

# create a data frame of all day-hour-region combinations
grid <- expand.grid(
  region = all_regions,
  date   = all_days,
  hour   = 0:23,
  KEEP.OUT.ATTRS = FALSE,
  stringsAsFactors = FALSE
) |>
  mutate(
    hour_start = as.POSIXct(paste(date, sprintf("%02d:00:00", hour)), tz = TZ),
    hour_end   = hour_start + hours(1)
  )

# For each (region, date), restrict calls to those dispatched that date
concurrency_results <- grid |>
  group_by(region, date) |>
  group_modify(function(.df, key) {
    r <- key$region[1]; d <- key$date[1]

    inc <- ems_clean |>
      filter(region == r, dispatch_date == d)

    if (nrow(inc) == 0) {
      .df$max_concurrency <- 0L
      .df$num_calls <- 0L
      return(.df)
    }

    starts <- inc$dispatch_time
    ends   <- inc$available_time

    # Use PADDED intervals for both metrics
    .df$max_concurrency <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ max_conc_in_window_padded(starts, ends, ..1, ..2, PAD_MIN)
    )

    .df$num_calls <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ {
        s_pad <- starts - minutes(PAD_MIN)
        e_pad <- ends   + minutes(PAD_MIN)
        sum(s_pad < ..2 & e_pad > ..1)
      }
    )

    .df
  }) |>
  ungroup() |>
  mutate(
    hour_of_day = hour(hour_start),
    day_of_week = wday(date, label = TRUE),
    month       = month(date, label = TRUE)
  )

# Region-level summaries (now dates and hour_start dates are consistent)
summary_stats <- concurrency_results |>
  group_by(region) |>
  summarise(
    mean_max_concurrency   = mean(max_concurrency),
    median_max_concurrency = median(max_concurrency),
    max_concurrency_overall = max(max_concurrency),
    total_hours            = n(),
    hours_with_calls       = sum(num_calls > 0),
    .groups = "drop"
  )

# print(summary_stats)

```

```{r label = "eda4", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

plot_daily_timeline_stacked <- function(data, target_date) {
  target_date <- as.Date(target_date, tz = TZ)

  daily_data <- data |>
    dplyr::filter(as.Date(dispatch_time, tz = TZ) == target_date) |>
    dplyr::arrange(region, dispatch_time) |>
    dplyr::group_by(region) |>
    dplyr::mutate(
      call_id        = dplyr::row_number(),
      # Actual (day-anchored)
      start_time     = as.POSIXct(paste(target_date, format(dispatch_time, "%H:%M:%S")), tz = TZ),
      end_time_raw   = as.POSIXct(paste(target_date, format(available_time, "%H:%M:%S")), tz = TZ),
      end_time       = dplyr::if_else(available_time < dispatch_time,
                                      end_time_raw + lubridate::days(1),
                                      end_time_raw),
      # Padded (day-anchored, with spillover handling)
      start_time_pad = start_time - lubridate::minutes(PAD_MIN),
      end_time_pad0  = end_time + lubridate::minutes(PAD_MIN),
      end_time_pad   = end_time_pad0,  # (already aligned to end_time which handled cross-midnight)
      duration_mins  = as.numeric(difftime(available_time, dispatch_time, units = "mins"))
    ) |>
    dplyr::ungroup()

  if (nrow(daily_data) == 0) {
    cat("No dispatches found on", as.character(target_date), "across any region.\n")
    return(invisible(NULL))
  }

  region_order <- daily_data |>
    dplyr::distinct(region) |>
    dplyr::arrange(region) |>
    dplyr::pull(region)

  daily_data$region <- factor(daily_data$region, levels = region_order)

  region_sizes <- daily_data |>
    dplyr::count(region, name = "n_calls") |>
    dplyr::arrange(region) |>
    dplyr::mutate(offset = dplyr::lag(cumsum(n_calls), default = 0L))

  daily_data <- daily_data |>
    dplyr::left_join(region_sizes, by = "region") |>
    dplyr::mutate(y_pos = offset + call_id)

  label_positions <- region_sizes |>
    dplyr::mutate(y_mid = offset + ceiling(n_calls / 2))

  x_min <- as.POSIXct(paste(target_date, "00:00:00"), tz = TZ)
  x_max <- max(
    max(daily_data$end_time_pad, na.rm = TRUE),
    as.POSIXct(paste(target_date, "23:59:59"), tz = TZ)
  )
  max_y <- max(daily_data$y_pos, na.rm = TRUE)

  library(ggplot2)
  p <- ggplot(daily_data, aes(y = y_pos, color = region)) +
    # --- PADDED interval (background) ---
    geom_segment(
      aes(x = start_time_pad, xend = end_time_pad, yend = y_pos),
      linewidth = 4, alpha = 0.25
    ) +
    geom_point(aes(x = start_time_pad), size = 3, alpha = 0.25) +
    geom_point(aes(x = end_time_pad),   size = 3, alpha = 0.25) +
    # --- ACTUAL interval (foreground) ---
    geom_segment(
      aes(x = start_time, xend = end_time, yend = y_pos),
      linewidth = 2, alpha = 0.9
    ) +
    geom_point(aes(x = start_time), size = 1.8) +
    geom_point(aes(x = end_time),   size = 1.8) +
    # Separators & region labels
    geom_hline(
      data = subset(region_sizes, offset > 0),
      aes(yintercept = offset + 0.5),
      linewidth = 0.3, alpha = 0.5, inherit.aes = FALSE
    ) +
    labs(
      title = "EMS Dispatch Timelines",
      subtitle = paste(format(target_date, "%B %d, %Y"), sprintf("— padding = ±%d min", PAD_MIN)),
      x = "Time of Day",
      y = "Call # (stacked)",
      color = "Region",
      caption = "Thick, faint band = padded window (±15m). Thin, solid band = actual call."
    ) +
    scale_x_datetime(
      limits = c(x_min, x_max),
      date_labels = "%H",
      date_breaks = "1 hour",
      expand = c(0.03, 0.03)
    ) +
    scale_y_continuous(
      breaks = seq(1, max_y, by = 1),
      minor_breaks = NULL,
      expand = c(0.02, 0.02)
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11),
      panel.grid.minor = element_blank(),
      axis.text.y = element_text(size = 7)
    )

  print(p)

  # Optional console summary (unchanged, still based on actual durations)
  # cat("\nSummary by region for", as.character(target_date), ":\n")
  # daily_data |>
  #   dplyr::group_by(region) |>
  #   dplyr::summarise(
  #     n_calls = dplyr::n(),
  #     avg_duration_min = round(mean(duration_mins), 1),
  #     med_duration_min = round(median(duration_mins), 1),
  #     min_duration_min = round(min(duration_mins), 1),
  #     max_duration_min = round(max(duration_mins), 1),
  #     .groups = "drop"
  #   ) |> print(n = Inf)
  # 
  # invisible(daily_data)
}

# Example:
plot_daily_timeline_stacked(ems_clean, "2024-01-16")

```

\newpage

**Figure 5: Overburdening by Region**

```{r, include=FALSE}

concurrency_results |>
  ggplot(aes(x = max_concurrency)) +
  geom_histogram() +
  facet_wrap(~ region)

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

mean(concurrency_central$overburdened)

```

```{r label = "eda5", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

counts <- concurrency_results |>
  mutate(max_concurrency = as.integer(max_concurrency)) |>
  count(region, max_concurrency, name = "n") |>
  # fill in missing combos so you always get 3 bars per x (even if some are 0)
  complete(region, max_concurrency = full_seq(range(max_concurrency, na.rm = TRUE), 1), fill = list(n = 0))

ggplot(counts, aes(x = factor(max_concurrency), y = n, fill = region)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.75) +
  labs(
    title = "Counts of Max Concurrency by Region",
    x = "Max concurrency (per hour)",
    y = "Number of hours"
  ) +
  theme_minimal()
```
