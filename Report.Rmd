---
title: "Vance County EMS Station Analysis Report"
author: "Alayna Binder, Cindy Ju, Kathleen Zhang"
date: "2025-10-21"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    toc: true
fontsize: 11pt
geometry: margin=1in
df_print: kable
---

<!-- NOTE: To reference a figure in the report body, do @ref(fig:NameOfFig). -->

# Background

Without an effective distribution of emergency medical resources, counties risk delayed response times that can mean the difference between life and death. To improve coverage across Vance County, researchers analyzed data from the county’s EMS system, which currently operates four ambulances from two stations located in the Central and South districts. Historical records show that residents in the North district face much longer response times, often averaging over 12 minutes, compared to 6 and 9 minutes in the Central and South regions.

Using a dataset of recorded EMS trips containing call locations, dispatch and arrival times, and Google API travel estimates, this analysis evaluates how different station configurations and vehicle allocations affect system performance. The county is considering establishing a new station in the North district, with two potential site options (Near North and Far North), and several ambulance distribution scenarios.

Our goal is to determine which North station location would most effectively reduce response times and how the four available ambulances should be allocated among the stations to balance coverage and minimize system strain. We aim to illustrate this through visual and numerical summaries that highlight tradeoffs in travel time and resource availability across the scenarios.

# Data and Exploratory Analysis

## Data

<!-- Talk about Duke Hospital outlier removed, how we handled NAs, etc., add reasoning on why we chose UA -->

We were given 489 observations of calls occurring in Vance County between January 1st to January 25th in 2024.

One of the calls went to Duke Hospital, which we excluded. Additionally, in certain incidents where multiple patients were involved, they would be transported in the same ambulance. As we only care about the availability of ambulances and not how many patients were in a single ambulance, we cleaned the dataset so that every identical row represented a single, unique incident involving one ambulance. The identical condition ensured datapoints where multiple ambulances were sent out for one incident with multiple patients would still be in the dataset, as it would impact ambulance load.

## Exploratory Data Analysis

To understand where and when ambulance demand occurs, our EDA began with plotting a call-density map (Figure 1) that showed a clear cluster around the Central station with spread in the South and scatter in the North, motivating scenarios that add northern coverage. We then compared Near North vs. Far North (Figure 2) calls, and found that using Google's UA travel times, Near North was closer for 90% of northern calls, cut response time by about 4 minutes on average relative to Far North, and had shorter total call duration with quicker transportation to hospitals.

# Modeling

## Simulating the Data

The objectives are to compare different scenarios to determine the optimal options for station placement as well as ambulance allocations, but we do not have data about ambulance response times under different station locations and ambulance allocations. As a result, we conducted a simulation that would simulate response times for each of the five scenarios for every single call in our cleaned dataset, resulting in 5 observations for every original call.

We developed Scenario Dispatch Rules that would govern how the simulation responded to different scenarios.

From our exploratory data analysis, we determined that we wanted to consider scenarios where an ambulance might not be available. Therefore, in our rules, each incoming call first checks whether any ambulances are currently available. If at least one unit is free, the system assigns the closest available unit — the one with the smallest estimated travel time, based on Google’s best-guess ETA between that station and the call location, resulting in 0 wait time. If all units are busy, the call enters a queue and waits until the next ambulance becomes free. That waiting time is then added to the total response time. Once assigned, each unit stays occupied for the observed duration of the call, and then becomes available again for the next incident.

The wait time is determined by whether or not an ambulance unit is available at the moment an incident call comes in. This models the effect of queueing for service. The wait time is calculated based on two conditions:

If one or more units are already free when the call arrives, we assign the closest available unit, which is the one with minimum ETA. The unit is dispatched immediately at the call time, resulting in zero wait time.

Otherwise, if all units are busy when the call arrives, we assign the unit that is scheduled to become free soonest. The wait time is the time difference between the call time and the departure time, which is the time the assigned unit finally becomes free. This represents the queueing delay. In both cases, the unit's subsequent availability is updated by adding the observed service duration to its departure time, making the unit busy for the duration of the incident.

Finally, we calculate Total Response Time or simulated time as Wait Time + Travel Time.

We filtered the results to exclude extremely long simulated times (above 2000 seconds) to focus on more likely outcomes. We also added a variable, “switched”, which determines if the ambulance assigned in a new scenario (S1−S4) is different from the baseline assignment (S0).

## Model Selection and Rationale

We chose to use a linear mixed model with a fixed effect on Scenario, which allowed us to calculate and test the difference in the mean simulated response time between our five Scenarios (S0 through S4), as one of the primary objectives was determining changes in response time across different scenarios.

We also included a random intercept for Incident_ID, allowing every unique incident to have its own baseline average response time that differs from the overall mean, even before accounting for the Scenario.

We fit a preliminary model with just these two components, but it had a lot of heteroscedasticity in the residuals plot. Therefore, we added a variance function that allows the remaining unexplained variability in response time to be different for each Scenario. If a scenario has a high residual variance, it means that even after controlling for the mean, its response times are highly unpredictable or erratic. Meanwhile, if a scenario has a low residual variance, it indicates highly consistent service performance, which is generally desirable in a real-world context.

Thus, our final model was:

$$\text{sim\_time}_{ij} = \beta_0 + \sum_{k=1}^4 \beta_k \cdot I(\text{Scenario}_j = S_k) + u_i + \epsilon_{ij} $$

$$
u_i \sim N(0, \sigma^2_u)\text{, }\epsilon_{ij} \sim N(0, \sigma^2)
$$

where $\sigma^2_j = \sigma^2 \cdot\delta^2_j$

$\text{sim_time}_{ij}$ is the simulated response time for the $i$-th Incident ID under the $j$-th Scenario.

$u_i$ is the random intercept for the $i$-th Incident ID. $\sigma^2_j$ is the residual variance specific to the $j$-th Scenario, which is impacted by our variance parameter $\delta_j^2$ that scales the baseline variance for Scenario $j$.

## Model Implementation

Linear mixed models were fit in R using the `nlme` package. Additionally, we ended up dropping data where simulated time was the same across all scenarios in order to focus on the effect of changes resulting from the different Scenarios.

## Model Evaluation

Dropping the data where simulated time was the same across all scenarios resulted in a lower AIC and BIC of the model fitted on the reduced dataset compared to the non-reduced dataset.

We also examined a QQ plot which appears approximately normal. Our Normalized Residuals vs Fitted Values plot of the total response time appears randomly distributed.

We also looked at the Residuals vs Fitted by Scenario, where the points in red indicate calls where the assigned station under the simulation was different from the assigned station in our original dataset.

# Model Results

The intercept term is the estimated mean simulated total response time for the reference group, Scenario 0, which is the current distribution of ambulances and stations. The mean response time in S0 is approximately 470.22 seconds or 7 minutes, 50 seconds. This value is highly significant, with a p-value \< 0.001 .

The mean simulated total response time in Scenario 3 is estimated to be 60.96 seconds faster than in Scenario 0, with this being a significant difference.

# Conclusion, Shortcomings and Future Work

## Conclusion

After evaluating five ambulance deployment layouts across the county by replaying the same 489 incidents under each scenario with a dispatch rule (sending the closest available unit, if none are free, dispact the next free unit when it is available), our analysis showed that Scenario 3, which locates one ambulance in the Near North, two ambulances in the Central, and one ambulance in the South, perfomed best. Across mean and median response times, Scenario 3 reduced average response time the most and produced the highest share of calls meeting eight and ten minute targets. These results are consistent with intuition, where a more balanced coverage of the county opposed to a centralized fleet lowers typical responses and extreme delays.

## Limitations and Future Work

However, one limitation to our analysis is the travel-time realism- we used Google's unadjusted ETAs and treated them as fixed, so rush hour, weather, and road disruptions are not modeled, which underestimates our variability. Additionally, there was no priority handling and all calls were treated the same, so our model is limited in analyzing how our layouts affect critical cases vs. low-priority calls. In the future, additional work would involve adding time-of-day factors to travel by multiplying ETAs by peak/off-peak multipliers, and running two queues for emergency and non-emergency, allowing emergencies to jump ahead of any waiting non-emergency calls when waiting for the next dispatch.

# Appendix

## Tables and Figures

<!-- Make sure to caption all tables/figures. -->

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(ggrepel)
library(xtable)
library(knitr)
library(dplyr)
library(kableExtra)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
load("emsData.RData")

ems <- x |>
  select(-c( # remove unneeded columns
    eTT.Pe.So, eTT.Pe.Ce, eTT.Pe.NN, eTT.Pe.FN,
    eTT.BG.So, eTT.BG.Ce, eTT.BG.NN, eTT.BG.FN,
    eTT.Op.So, eTT.Op.Ce, eTT.Op.NN, eTT.Op.FN
  )) |>
  mutate(
    BASE.NAME = recode(BASE.NAME,
                       "Company 9" = "Central",
                       "Company 1" = "South"),
    BASE.NAME = factor(BASE.NAME, levels = c("Central","South")),
    REF.GRID = case_when(
      str_detect(REF.GRID, "North")   ~ "North",
      str_detect(REF.GRID, "Central") ~ "Central",
      str_detect(REF.GRID, "South")   ~ "South",
      TRUE ~ REF.GRID
    ),
    REF.GRID = factor(REF.GRID, levels = c("South","Central","North")),
    DISPATCH.PRIORITY.NAME = recode(DISPATCH.PRIORITY.NAME,
                                    "Non Emergency" = "Non-emergency",
                                    "Emergency" = "Emergency"),
    DISPATCH.PRIORITY.NAME = factor(DISPATCH.PRIORITY.NAME,
                                    levels = c("Emergency","Non-emergency")),
    went_hospital = REC.NAME != ""
  ) |>
  mutate(
    disp_to_enroute_min = as.numeric(timeToEnroute, units = "mins"),
    response_time_min = as.numeric(observedTT,    units = "mins"),
    on_scene_min = as.numeric(onSceneDur,    units = "mins"),
    to_hospital_min = as.numeric(toHospitalTT,  units = "mins"),
    at_hospital_min = as.numeric(atHospitalDur, units = "mins"),
    total_call_min = as.numeric(dispToClearTime, units = "mins")
  )
```

**Figure One: Density of Calls in Vance County**

```{r label = "eda1", echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "EMS call density across Vance County (geographically)", fig.show = 'hold'}

ems_plot <- data.frame(
  lon = as.numeric(x$REF.GPS.LON),
  lat = as.numeric(x$REF.GPS.LAT)
) |> na.omit() |>
  subset(lon > -78.55 & lon < -78.25 & lat > 36.15 & lat < 36.55)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

stations <- data.frame(
  name = c("South","Central"),
  lat  = c(36.2765, 36.3450),
  lon  = c(-78.4004, -78.3905)
)

ggplot(ems_plot, aes(lon, lat)) +
  # hex bins look nicer than squares
  geom_hex(bins = 35, alpha = 0.95) +
  coord_fixed() +
  scale_fill_viridis_c(trans = "sqrt", name = "Calls") +
  # stations: clearer markers + labels
  geom_point(data = stations, aes(lon, lat),
             inherit.aes = FALSE, shape = 21, size = 3.8,
             stroke = 1, colour = "black", fill = "white") +
  geom_label_repel(data = stations, aes(lon, lat, label = name),
                   inherit.aes = FALSE, size = 3,
                   label.padding = unit(0.12, "lines"),
                   label.size = 0, seed = 1) +
  labs(title = "EMS call density",
       x = "Longitude", y = "Latitude") +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", size = 16),
    legend.key.height = unit(0.6, "cm")
  )
```

\newpage

```{r label = "eda2_1", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

ems <- ems |>
  mutate(
    closest_current = if_else(eTT.UA.Ce <= eTT.UA.So, "Central", "South"),
    closest_current = factor(closest_current, levels = c("South","Central")),
    closer_north = case_when(
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN <= eTT.UA.FN ~ "Near North",
      !is.na(eTT.UA.NN) & !is.na(eTT.UA.FN) & eTT.UA.NN >  eTT.UA.FN ~ "Far North",
      TRUE ~ NA_character_
    ),
    closer_north = factor(closer_north, levels = c("Near North","Far North"))
  )

# how often Near vs Far is closer for North calls
north_choice <- ems |>
  filter(REF.GRID == "North") |>
  count(closer_north) |>
  mutate(pct = n / sum(n))

```

**Figure Two: Comparison of Near vs. Far North Demand**

```{r label = "eda2_2", echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "test", fig.show = 'hold'}

ems %>%
  filter(REF.GRID == "North") %>%
  select(closer_north, response_time_min, to_hospital_min, total_call_min) %>%
  pivot_longer(-closer_north, names_to = "stage", values_to = "minutes") %>%
  mutate(stage = recode(stage,
                        response_time_min = "Response Time",
                        to_hospital_min   = "Transport to Hospital",
                        total_call_min    = "Total Call Duration")) %>%
  ggplot(aes(x = closer_north, y = minutes, fill = closer_north)) +
  geom_boxplot(width = 0.6, outlier.alpha = 0.15) +
  facet_wrap(~ stage, scales = "free_y", nrow = 1) +
  scale_fill_manual(values = c("Near North" = "#1f77b4",
                               "Far North"  = "#ff7f0e")) +
  labs(
    x = NULL, 
    y = "Minutes", 
    title = "North Calls: Near vs. Far North"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "grey40")
  )
```

**Figure Three: Percentage of Ambulance Utilization**

```{r, include=FALSE}

# --- 0) Clean datetimes & keep valid intervals ---
ems <- ems %>%
  mutate(
    DT.DISP      = as_datetime(DT.DISP),
    DT.AVAILABLE = as_datetime(DT.AVAILABLE)
  ) %>%
  filter(!is.na(VEH.GRID), !is.na(DT.DISP), !is.na(DT.AVAILABLE),
         DT.AVAILABLE >= DT.DISP)

# If two calls for the same ambulance overlap (due to logging quirks, delays, or bad timestamps), the code double-counts that unit as busy twice at the same time.
ems %>%
  arrange(VEH.GRID, DT.DISP) %>%
  group_by(VEH.GRID) %>%
  mutate(overlap = DT.DISP < lag(DT.AVAILABLE)) %>%
  filter(overlap == TRUE)

# Distinct vehicles present (sanity)
n_units <- n_distinct(ems$VEH.GRID)
if (n_units != 4) message("Note: dataset has ", n_units, " distinct VEH.GRID values.")

# Observation window (for utilization denominators)
t_start <- min(ems$DT.DISP, na.rm = TRUE)
t_end   <- max(ems$DT.AVAILABLE, na.rm = TRUE)
obs_seconds <- as.numeric(t_end - t_start, units = "secs")

# --- 1) Merge overlapping intervals within each vehicle ---
# Convert to numeric seconds for cummax, then back to POSIXct
ems_intervals <- ems %>%
  transmute(VEH.GRID,
            start_num = as.numeric(DT.DISP),
            end_num   = as.numeric(DT.AVAILABLE)) %>%
  arrange(VEH.GRID, start_num, end_num) %>%
  group_by(VEH.GRID) %>%
  mutate(
    prev_cummax_end = lag(cummax(end_num), default = first(start_num) - 1),
    new_block = start_num > prev_cummax_end,
    block_id  = cumsum(new_block)
  ) %>%
  group_by(VEH.GRID, block_id) %>%
  summarise(start_num = min(start_num),
            end_num   = max(end_num),
            .groups = "drop") %>%
  mutate(start = as_datetime(start_num),
         end   = as_datetime(end_num)) %>%
  select(VEH.GRID, start, end) %>%
  arrange(VEH.GRID, start, end)

# --- 2) Build global event timeline (+1 at start, -1 at end) ---
events <- ems_intervals %>%
  pivot_longer(c(start, end), names_to = "etype", values_to = "time") %>%
  mutate(delta = if_else(etype == "start", 1L, -1L)) %>%
  arrange(time, desc(delta))   # process -1 before +1 at identical timestamps

timeline <- events %>%
  transmute(time, delta) %>%
  arrange(time) %>%
  mutate(active = cumsum(delta),
         time_next = lead(time),
         seg_sec   = as.numeric(time_next - time, "secs")) %>%
  filter(!is.na(time_next), seg_sec > 0)

# --- 3) Duration-weighted summary of simultaneous busy vehicles (capped at 4) ---
timeline <- timeline %>% mutate(active_capped = pmin(active, 4L))

simul_summary <- timeline %>%
  group_by(active = active_capped) %>%
  summarise(total_sec = sum(seg_sec), .groups = "drop") %>%
  mutate(pct_of_time = total_sec / sum(total_sec)) %>%
  arrange(active)

simul_summary
# active should now be 0..4 only

simul_summary %>%
  mutate(
    `Active ambulances` = active,
    `Total time (hours)` = round(total_sec / 3600, 1),
    `% of time` = scales::percent(pct_of_time, accuracy = 0.1)
  ) %>%
  select(`Active ambulances`, `Total time (hours)`, `% of time`) %>%
  knitr::kable(format = "latex",
               align = "c",
               booktabs = TRUE,
               caption = "System Load: Simultaneous Busy Ambulances") %>%
  kableExtra::kable_styling(full_width = FALSE, position = "center",
                            latex_options = c("hold_position","scale_down","striped"))

# Plot
ggplot(simul_summary, aes(factor(active), pct_of_time)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Simultaneous busy ambulances", y = "% of observed time",
       title = "System load: how many ambulances are busy simultaneously (capped at 4)")

# --- 5) Per-unit utilization (unchanged) ---
util_by_unit <- ems %>%
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) %>%
  group_by(VEH.GRID) %>%
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups = "drop") %>%
  mutate(utilization = busy_sec / obs_seconds) %>%
  arrange(desc(utilization))

util_by_unit

# util_by_unit should already have: VEH.GRID, busy_sec, utilization
# If you also want color by base, grab a base label for each unit (mode per unit)
unit_base <- ems %>%
  count(VEH.GRID, BASE.NAME) %>%
  group_by(VEH.GRID) %>%
  slice_max(n, with_ties = FALSE) %>%
  ungroup() %>%
  select(VEH.GRID, BASE.NAME)

util_plot <- util_by_unit %>%
  left_join(unit_base, by = "VEH.GRID") %>%
  mutate(
    VEH.GRID = factor(VEH.GRID, levels = rev(VEH.GRID[order(utilization)])),
    label = scales::percent(utilization, accuracy = 0.1)
  )

# medic and company matching 
by_medic_base <- ems |>
  count(VEH.GRID, BASE.NAME, name = "n_calls") |>
  group_by(VEH.GRID) |>
  mutate(pct = n_calls / sum(n_calls)) |>
  arrange(VEH.GRID, desc(n_calls)) |>
  ungroup()

# quick look just for Company 1
by_medic_base |> filter(BASE.NAME == "Company 1")

primary_base <- by_medic_base |>
  group_by(VEH.GRID) |>
  slice_max(pct, with_ties = FALSE) |>
  ungroup() |>
  select(VEH.GRID, primary_base = BASE.NAME, share_at_primary = pct)

util_by_base <- ems |>
  mutate(call_sec = as.numeric(DT.AVAILABLE - DT.DISP, "secs")) |>
  group_by(BASE.NAME) |>
  summarise(busy_sec = sum(call_sec, na.rm = TRUE), .groups="drop") |>
  mutate(utilization = busy_sec / obs_seconds)

ggplot(by_medic_base, aes(x = reorder(VEH.GRID, -n_calls), y = pct, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Medic (VEH.GRID)", y = "Share of calls by base",
       title = "Where each medic runs from (call share by base)") +
  coord_flip() + theme_minimal(base_size = 12) +
  theme(panel.grid.major.y = element_blank(), legend.position = "top")
```

```{r label = "eda3", echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "", fig.show = 'hold'}

ggplot(util_plot, aes(x = VEH.GRID, y = utilization, fill = BASE.NAME)) +
  geom_col(width = 0.7, color = "grey20") +
  geom_text(aes(label = label), hjust = -0.1, size = 3.6, color = "grey10") +
  coord_flip(clip = "off") +
  scale_y_continuous(labels = scales::percent, expand = expansion(mult = c(0, 0.10))) +
  scale_fill_brewer(palette = "Set2", na.value = "grey70", name = "Base") +
  labs(
    title = "Per-unit utilization over the observation window",
    subtitle = "Percentage of time each ambulance was busy (dispatch → available)",
    x = "Ambulance (VEH.GRID)",
    y = "Utilization"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_text(margin = margin(r = 10)),
    axis.title.x = element_text(margin = margin(t = 6)),
    plot.title = element_text(face = "bold"),
    legend.position = "top",
    legend.title = element_text(face = "bold"),
    plot.margin = margin(10, 30, 10, 10) # room for labels on the right
  )
```

**Figure Four: Examining Overburdening Issue**

```{r, include=FALSE}

TZ <- "America/New_York"

ems_clean <- ems %>%
  filter(!is.na(DT.DISP), !is.na(DT.AVAILABLE), !is.na(REF.GRID)) %>%
  mutate(
    # Normalize to a single local timezone
    dispatch_time   = with_tz(as.POSIXct(DT.DISP, tz = TZ), tzone = TZ),
    available_time  = with_tz(as.POSIXct(DT.AVAILABLE, tz = TZ), tzone = TZ),
    region          = REF.GRID
  ) %>%
  filter(dispatch_time < available_time)

PAD_MIN <- 15L

# Max concurrency in [win_start, win_end) using PADDED intervals
max_conc_in_window_padded <- function(starts, ends, win_start, win_end, pad_min = PAD_MIN) {
  if (length(starts) == 0) return(0L)
  starts_pad <- starts - minutes(pad_min)
  ends_pad   <- ends   + minutes(pad_min)

  keep <- (starts_pad < win_end) & (ends_pad > win_start)
  if (!any(keep)) return(0L)

  starts_pad <- pmax(starts_pad[keep], win_start)
  ends_pad   <- pmin(ends_pad[keep],   win_end)

  events <- cbind(
    time  = c(starts_pad, ends_pad),
    delta = c(rep(1L, length(starts_pad)), rep(-1L, length(ends_pad)))
  )
  events <- events[order(events[, "time"], -events[, "delta"]), , drop = FALSE]

  conc <- 0L; maxc <- 0L
  for (i in seq_len(nrow(events))) {
    conc <- conc + events[i, "delta"]
    if (conc > maxc) maxc <- conc
  }
  as.integer(maxc)
}

# all local dates by dispatch date
ems_clean <- ems_clean %>%
  mutate(dispatch_date = as.Date(dispatch_time, tz = TZ))

all_days   <- sort(unique(ems_clean$dispatch_date))
all_regions <- sort(unique(ems_clean$region))

# create a data frame of all day-hour-region combinations
grid <- expand.grid(
  region = all_regions,
  date   = all_days,
  hour   = 0:23,
  KEEP.OUT.ATTRS = FALSE,
  stringsAsFactors = FALSE
) %>%
  mutate(
    hour_start = as.POSIXct(paste(date, sprintf("%02d:00:00", hour)), tz = TZ),
    hour_end   = hour_start + hours(1)
  )

# For each (region, date), restrict calls to those dispatched that date
concurrency_results <- grid %>%
  group_by(region, date) %>%
  group_modify(function(.df, key) {
    r <- key$region[1]; d <- key$date[1]

    inc <- ems_clean %>%
      filter(region == r, dispatch_date == d)

    if (nrow(inc) == 0) {
      .df$max_concurrency <- 0L
      .df$num_calls <- 0L
      return(.df)
    }

    starts <- inc$dispatch_time
    ends   <- inc$available_time

    # Use PADDED intervals for both metrics
    .df$max_concurrency <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ max_conc_in_window_padded(starts, ends, ..1, ..2, PAD_MIN)
    )

    .df$num_calls <- purrr::pmap_int(
      list(.df$hour_start, .df$hour_end),
      ~ {
        s_pad <- starts - minutes(PAD_MIN)
        e_pad <- ends   + minutes(PAD_MIN)
        sum(s_pad < ..2 & e_pad > ..1)
      }
    )

    .df
  }) %>%
  ungroup() %>%
  mutate(
    hour_of_day = hour(hour_start),
    day_of_week = wday(date, label = TRUE),
    month       = month(date, label = TRUE)
  )

# Region-level summaries (now dates and hour_start dates are consistent)
summary_stats <- concurrency_results %>%
  group_by(region) %>%
  summarise(
    mean_max_concurrency   = mean(max_concurrency),
    median_max_concurrency = median(max_concurrency),
    max_concurrency_overall = max(max_concurrency),
    total_hours            = n(),
    hours_with_calls       = sum(num_calls > 0),
    .groups = "drop"
  )

print(summary_stats)

```

```{r label = "eda4", echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "", fig.show = 'hold'}

plot_daily_timeline_stacked <- function(data, target_date) {
  target_date <- as.Date(target_date, tz = TZ)

  daily_data <- data %>%
    dplyr::filter(as.Date(dispatch_time, tz = TZ) == target_date) %>%
    dplyr::arrange(region, dispatch_time) %>%
    dplyr::group_by(region) %>%
    dplyr::mutate(
      call_id        = dplyr::row_number(),
      # Actual (day-anchored)
      start_time     = as.POSIXct(paste(target_date, format(dispatch_time, "%H:%M:%S")), tz = TZ),
      end_time_raw   = as.POSIXct(paste(target_date, format(available_time, "%H:%M:%S")), tz = TZ),
      end_time       = dplyr::if_else(available_time < dispatch_time,
                                      end_time_raw + lubridate::days(1),
                                      end_time_raw),
      # Padded (day-anchored, with spillover handling)
      start_time_pad = start_time - lubridate::minutes(PAD_MIN),
      end_time_pad0  = end_time + lubridate::minutes(PAD_MIN),
      end_time_pad   = end_time_pad0,  # (already aligned to end_time which handled cross-midnight)
      duration_mins  = as.numeric(difftime(available_time, dispatch_time, units = "mins"))
    ) %>%
    dplyr::ungroup()

  if (nrow(daily_data) == 0) {
    cat("No dispatches found on", as.character(target_date), "across any region.\n")
    return(invisible(NULL))
  }

  region_order <- daily_data %>%
    dplyr::distinct(region) %>%
    dplyr::arrange(region) %>%
    dplyr::pull(region)

  daily_data$region <- factor(daily_data$region, levels = region_order)

  region_sizes <- daily_data %>%
    dplyr::count(region, name = "n_calls") %>%
    dplyr::arrange(region) %>%
    dplyr::mutate(offset = dplyr::lag(cumsum(n_calls), default = 0L))

  daily_data <- daily_data %>%
    dplyr::left_join(region_sizes, by = "region") %>%
    dplyr::mutate(y_pos = offset + call_id)

  label_positions <- region_sizes %>%
    dplyr::mutate(y_mid = offset + ceiling(n_calls / 2))

  x_min <- as.POSIXct(paste(target_date, "00:00:00"), tz = TZ)
  x_max <- max(
    max(daily_data$end_time_pad, na.rm = TRUE),
    as.POSIXct(paste(target_date, "23:59:59"), tz = TZ)
  )
  max_y <- max(daily_data$y_pos, na.rm = TRUE)

  library(ggplot2)
  p <- ggplot(daily_data, aes(y = y_pos, color = region)) +
    # --- PADDED interval (background) ---
    geom_segment(
      aes(x = start_time_pad, xend = end_time_pad, yend = y_pos),
      linewidth = 4, alpha = 0.25
    ) +
    geom_point(aes(x = start_time_pad), size = 3, alpha = 0.25) +
    geom_point(aes(x = end_time_pad),   size = 3, alpha = 0.25) +
    # --- ACTUAL interval (foreground) ---
    geom_segment(
      aes(x = start_time, xend = end_time, yend = y_pos),
      linewidth = 2, alpha = 0.9
    ) +
    geom_point(aes(x = start_time), size = 1.8) +
    geom_point(aes(x = end_time),   size = 1.8) +
    # Separators & region labels
    geom_hline(
      data = subset(region_sizes, offset > 0),
      aes(yintercept = offset + 0.5),
      linewidth = 0.3, alpha = 0.5, inherit.aes = FALSE
    ) +
    labs(
      title = "EMS Dispatch Timelines",
      subtitle = paste(format(target_date, "%B %d, %Y"), sprintf("— padding = ±%d min", PAD_MIN)),
      x = "Time of Day",
      y = "Call # (stacked)",
      color = "Region",
      caption = "Thick, faint band = padded window (±15m). Thin, solid band = actual call."
    ) +
    scale_x_datetime(
      limits = c(x_min, x_max),
      date_labels = "%H",
      date_breaks = "1 hour",
      expand = c(0.03, 0.03)
    ) +
    scale_y_continuous(
      breaks = seq(1, max_y, by = 1),
      minor_breaks = NULL,
      expand = c(0.02, 0.02)
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11),
      panel.grid.minor = element_blank(),
      axis.text.y = element_text(size = 7)
    )

  print(p)

  # Optional console summary (unchanged, still based on actual durations)
  cat("\nSummary by region for", as.character(target_date), ":\n")
  daily_data %>%
    dplyr::group_by(region) %>%
    dplyr::summarise(
      n_calls = dplyr::n(),
      avg_duration_min = round(mean(duration_mins), 1),
      med_duration_min = round(median(duration_mins), 1),
      min_duration_min = round(min(duration_mins), 1),
      max_duration_min = round(max(duration_mins), 1),
      .groups = "drop"
    ) %>% print(n = Inf)

  invisible(daily_data)
}

# Example:
plot_daily_timeline_stacked(ems_clean, "2024-01-16")

```

**Figure Five: Overburdening by Region**

```{r, include=FALSE}

concurrency_results |>
  ggplot(aes(x = max_concurrency)) +
  geom_histogram() +
  facet_wrap(~ region)

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

mean(concurrency_central$overburdened)

```

```{r label = "eda5", echo = FALSE, warning = FALSE, message = FALSE, fig.show = 'hold'}

concurrency_central <- concurrency_results |>
  filter(region == "2 Central") |>
  mutate(overburdened = if_else(max_concurrency > 2, 1, 0))

counts <- concurrency_results %>%
  mutate(max_concurrency = as.integer(max_concurrency)) %>%
  count(region, max_concurrency, name = "n") %>%
  # fill in missing combos so you always get 3 bars per x (even if some are 0)
  complete(region, max_concurrency = full_seq(range(max_concurrency, na.rm = TRUE), 1), fill = list(n = 0))

ggplot(counts, aes(x = factor(max_concurrency), y = n, fill = region)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.75) +
  labs(
    title = "Counts of Max Concurrency by Region",
    x = "Max concurrency (per hour)",
    y = "Number of hours"
  ) +
  theme_minimal()
```
